{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9638eb19-0659-4771-9039-83ac038e9e90",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac580870-ced0-4d91-882f-e144aec1a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import IterableDataset\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "\n",
    "import xarray as xr\n",
    "import rioxarray as rio\n",
    "import pickle\n",
    "import rasterio\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from utils.data_utils import load_data, convert_tr_dt_to_dt, \\\n",
    "        process_lulc_data, remap_lulc_data, get_projection_string\n",
    "from utils.model_utils import get_activation_function, get_optimizer\n",
    "# Test data\n",
    "from utils.test_uq import eval, eval_with_mcd\n",
    "# Infer data\n",
    "from utils.inference import run_inference\n",
    "from utils.post_imputation import process_imputed_data, process_imputed_data_uq\n",
    "from utils.visuals import create_plots\n",
    "from utils.create_raster import create_xr, save_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06687445-ece8-49b2-9fb2-47681cb28529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, xdata, ydata, mask, binary_mask, month_data, season_data):\n",
    "        self.xdata = torch.tensor(np.nan_to_num(xdata, nan=-1.0), dtype=torch.float32)\n",
    "        self.ydata = torch.tensor(np.nan_to_num(ydata, nan=-1.0), dtype=torch.float32)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.binary_mask = torch.tensor(binary_mask, dtype=torch.float32)\n",
    "        self.month_data = torch.tensor(month_data, dtype=torch.long)  \n",
    "        self.season_data = torch.tensor(season_data, dtype=torch.long)\n",
    "        # self.lulc_data = torch.tensor(lulc_data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ydata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         return {\n",
    "                    'X': self.xdata[idx],\n",
    "                    'Y': self.ydata[idx],\n",
    "                    'M': self.mask[idx],\n",
    "                    'binary_mask': self.binary_mask,\n",
    "                    'month': self.month_data[idx], \n",
    "                    'season': self.season_data[idx]\n",
    "                    # 'lulc': self.lulc_data\n",
    "                }\n",
    "        \n",
    "def stream_data_loader(dataset, batch_size=1, shuffle=True):\n",
    "    worker_init_fn = lambda _: np.random.seed()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, worker_init_fn=worker_init_fn)\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "\n",
    "class CustomIterableDataset(IterableDataset):\n",
    "    def __init__(self, xdata, ydata, mask, binary_mask, month_data, season_data, lulc_data, date_data):\n",
    "        # lulc_data\n",
    "        self.xdata = torch.tensor(np.nan_to_num(xdata, nan=0.0), dtype=torch.float32)\n",
    "        self.ydata = torch.tensor(np.nan_to_num(ydata, nan=0.0), dtype=torch.float32)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        self.binary_mask = torch.tensor(binary_mask, dtype=torch.float32)\n",
    "        # self.binary_mask = torch.tensor(binary_mask, dtype=torch.bool)\n",
    "        self.month_data = torch.tensor(month_data, dtype=torch.long)\n",
    "        self.season_data = torch.tensor(season_data, dtype=torch.long)\n",
    "        self.lulc_data = torch.tensor(lulc_data, dtype=torch.long)\n",
    "        self.date_data = np.array([date.timestamp() for date in date_data])  \n",
    "        \n",
    "    def process_item(self, idx):\n",
    "        return {\n",
    "            'X': self.xdata[idx],\n",
    "            'Y': self.ydata[idx],\n",
    "            'M': self.mask[idx],\n",
    "            'binary_mask': self.binary_mask,\n",
    "            'month': self.month_data[idx], \n",
    "            'season': self.season_data[idx],\n",
    "            'date': self.date_data[idx],\n",
    "            'lulc': self.lulc_data\n",
    "        }\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  \n",
    "            iter_start = 0\n",
    "            iter_end = len(self.ydata)\n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            per_worker = int(math.ceil((len(self.ydata) / float(worker_info.num_workers))))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(self.ydata))\n",
    "\n",
    "        return (self.process_item(idx) for idx in range(iter_start, iter_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103e31b-6ce5-4751-9f20-2314c5da3fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLULCEmbedding(nn.Module):\n",
    "    def __init__(self, num_lulc_classes, lulc_embed_dim):\n",
    "        super().__init__()\n",
    "        self.lulc_embedding = nn.Embedding(num_lulc_classes, lulc_embed_dim)\n",
    "        # Explicitly zero out the first embedding vector which is typically used for padding\n",
    "        self.lulc_embedding.weight.data[0].fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lulc_embedding(x)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.lulc_embedding.weight\n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, covariate_channels, activation_fn, \n",
    "                 num_lulc_classes, lulc_embed_dim=256,\n",
    "                 num_months=12, month_embed_dim=24, \n",
    "                 num_seasons=4, season_embed_dim=8, \n",
    "                 dropout_rate = 0.2):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Embedding for months\n",
    "        self.month_embedding = nn.Embedding(num_months, month_embed_dim)\n",
    "        # Embedding for seasons\n",
    "        self.season_embedding = nn.Embedding(num_seasons, season_embed_dim)\n",
    "        # Embedding for LULC       \n",
    "        self.lulc_embedding = CustomLULCEmbedding(num_lulc_classes, lulc_embed_dim)\n",
    "        print('in, covariate, month, season, lulc')\n",
    "        print(in_channels, covariate_channels, month_embed_dim, season_embed_dim, lulc_embed_dim) \n",
    "        total_channels = in_channels + 1 + covariate_channels   + month_embed_dim + season_embed_dim + lulc_embed_dim\n",
    "        \n",
    "        # Define the generator architecture with convolutional, batch normalization, and ReLU layers in sequence\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=total_channels, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate), \n",
    "\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate), \n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(negative_slope = 0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate), \n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, corrupted_data, mask, covariates,  months, seasons, lulc_data, y_mask):\n",
    "        # Extract height and width from corrupted_data\n",
    "        height = corrupted_data.size(2)\n",
    "        width = corrupted_data.size(3)\n",
    "\n",
    "        # Ensure y_mask is of type long for compatibility with embedding tensors\n",
    "        y_mask = y_mask.to(torch.long)\n",
    "         \n",
    "        # Get month embeddings\n",
    "        month_embeddings = self.month_embedding(months)  # Shape: [batch_size, month_embed_dim]\n",
    "        month_embeddings = month_embeddings.view(-1, month_embeddings.shape[1], 1, 1).repeat(1, 1, height, width)\n",
    "\n",
    "        # Get season embeddings\n",
    "        season_embeddings = self.season_embedding(seasons)  # Shape: [batch_size, season_embed_dim]\n",
    "        season_embeddings = season_embeddings.view(-1, season_embeddings.shape[1], 1, 1).repeat(1, 1, height, width)\n",
    "\n",
    "        # Get LULC embeddings\n",
    "        lulc_embeddings = self.lulc_embedding(lulc_data)\n",
    "        lulc_embeddings = lulc_embeddings.permute(0, 3, 1, 2) \n",
    "\n",
    "        # Concatenate mask and covariates with corrupted data along the channel dimension\n",
    "        x = torch.cat([corrupted_data, mask, covariates, month_embeddings, season_embeddings, lulc_embeddings], dim=1)\n",
    "        # Pass the concatenated input through the generator network\n",
    "        x = self.gen(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, activation_fn, dropout_rate = 0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define the discriminator architecture with convolutional, batch normalization, and ReLU layers in sequence\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=8, kernel_size=3, stride = 1, padding=1),\n",
    "            activation_fn,\n",
    "            nn.Dropout2d(dropout_rate), \n",
    "\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride = 1, padding=1),\n",
    "            activation_fn,\n",
    "            nn.Dropout2d(dropout_rate), \n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride = 1, padding=1),\n",
    "            activation_fn,\n",
    "            nn.Dropout2d(dropout_rate), \n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride = 1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, combined_input, hint):\n",
    "        # Concatenate the combined input and hint along the channel dimension\n",
    "        x = torch.cat([combined_input, hint], dim=1)\n",
    "        # Pass the concatenated input through the discriminator network\n",
    "        x = self.disc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c6966-b3e1-4e3c-bbc3-2deb9ced18ae",
   "metadata": {},
   "source": [
    "## Initialize file path and projection file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeab792-28fe-4a24-aa75-9ecf9ac03b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict_region = {'Northeast': 'NE',\n",
    "             'Upper Midwest': 'UM',\n",
    "             'Ohio Valley': 'OV',\n",
    "             'Southeast': 'SE',\n",
    "             'South': 'S',\n",
    "             'Southwest':'SW',\n",
    "             'Northern Rockies': 'NR',\n",
    "             'West': 'W',\n",
    "             'Northwest': 'NW'}\n",
    "\n",
    "\n",
    "region_name = 'Northeast'\n",
    "region_abbr = abbr_dict_region.get(region_name)\n",
    "\n",
    "# Read and process LULC data\n",
    "lulc_filepath = f'/opt/yarn/ml-model/Ancillary_files/LULC/{region_name}/{region_abbr}_LULC.tif'\n",
    "output_pkl =  f'/datastorage/Output/TrainingData/Pickle/{region_name}/Imputation/'\n",
    "tif_path = f'/datastorage/Output/MODIS-MAIAC/Imputed/{region_name}/Version3/'\n",
    "\n",
    "prj_file = '/opt/yarn/ml-model/Ancillary_files/Projection/lambert_conformal.prj'\n",
    "prj_str = get_projection_string(prj_file)\n",
    "crs = CRS.from_wkt(prj_str)\n",
    "proj_string = crs.to_proj4()\n",
    "\n",
    "p_hint = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d479c80-831f-47aa-bd80-77b3722ba329",
   "metadata": {},
   "source": [
    "## uq metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df0fd0-c9ff-40c5-bba2-d51a330dad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score90 = 1.645\n",
    "z_score95 = 1.96\n",
    "z_score99 = 2.576"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35e863-e6bf-4f69-860d-1a3cfa694898",
   "metadata": {},
   "source": [
    "## Read pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962dbcc-9b11-4298-80d3-9d20c357094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "# x_train_model = load_data(f'{output_pkl}/{region_abbr}_X_Train.pkl')\n",
    "# x_train_model = x_train_model[0]\n",
    "# y_train_model, miss_y_train_model = load_data(f'{output_pkl}/{region_abbr}_Y_Train.pkl')\n",
    "# y_train_model = np.where((y_train_model == -9999) | (y_train_model == -1.0), np.nan, y_train_model)\n",
    "\n",
    "# x_val_model = load_data(f'{output_pkl}/{region_abbr}_X_Validation.pkl')\n",
    "# x_val_model = x_val_model[0]\n",
    "# y_val_model, miss_y_val_model = load_data(f'{output_pkl}/{region_abbr}_Y_Validation.pkl')\n",
    "# y_val_model = np.where((y_val_model == -9999) | (y_val_model == -1.0), np.nan, y_val_model)\n",
    "\n",
    "\n",
    "x_test_model = load_data(f'{output_pkl}/{region_abbr}_X_Test.pkl')\n",
    "x_test_model = x_test_model[0]\n",
    "y_test_model, miss_y_test_model = load_data(f'{output_pkl}/{region_abbr}_Y_Test.pkl')\n",
    "y_test_model = np.where((y_test_model == -9999) | (y_test_model == -1.0), np.nan, y_test_model)\n",
    "\n",
    "d_train, d_test, d_val = load_data(f'{output_pkl}/{region_abbr}_DATE_TTV.pkl')\n",
    "season_train, season_test, season_val = load_data(f'{output_pkl}/{region_abbr}_Season_TTV.pkl')\n",
    "month_train, month_test, month_val = load_data(f'{output_pkl}/{region_abbr}_Month_TTV.pkl')\n",
    "data_bm = load_data(f'{output_pkl}/{region_abbr}_BM.pkl')\n",
    "latitude, longitude = load_data(f'{output_pkl}/{region_abbr}_Grid.pkl')\n",
    "# y_scaler = load_data(f'{output_pkl}Y_scaler.pkl')\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d206e2-8d5f-4e8a-b183-813397a7a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_model.shape, y_train_model.shape, data_bm.shape)\n",
    "t = 120\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# Plot x_train_model in the first subplot\n",
    "axes[0].imshow(x_train_model[t, 0, :])\n",
    "axes[0].set_title(\"X - covariate\")\n",
    "\n",
    "# Plot y_train_model in the second subplot\n",
    "axes[1].imshow(y_train_model[t, 0, :])\n",
    "axes[1].set_title(\"y - target\")\n",
    "\n",
    "# Plot y_train_model in the second subplot\n",
    "axes[2].imshow(miss_y_train_model[t, 0, :])\n",
    "axes[2].set_title(\"y - mask\")\n",
    "\n",
    "# Plot y_train_model in the second subplot\n",
    "axes[3].imshow(data_bm[0, :])\n",
    "axes[3].set_title(\"study area mask\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7bfc0-7843-4dfc-a84b-a5e5e4ddc9a0",
   "metadata": {},
   "source": [
    "## Read LULC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2626804-d999-45c9-b4c3-9bcb4e4a8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_data, class_to_index = process_lulc_data(lulc_filepath)\n",
    "lulc_data_remapped = remap_lulc_data(lulc_data, class_to_index)\n",
    "\n",
    "lulc_remap_values = np.unique(lulc_data_remapped, return_counts = True)\n",
    "lulc_actual_values = np.unique(lulc_data, return_counts = True)\n",
    "\n",
    "num_lulc_classes = len(lulc_remap_values[0])\n",
    "lulc_remap_values, lulc_actual_values, num_lulc_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413acb3-08d4-4000-adf8-c3292c4092ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lulc_data_remapped_int = lulc_data_remapped.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56637131-b166-45a2-97d7-d5323da70e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lulc_data_remapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0168710-0547-4475-89e7-a0ee80b9d039",
   "metadata": {},
   "source": [
    "## Datasets and IterableDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d049904-8d69-4f9a-8bcf-c4063451096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust month data and create datasets\n",
    "# adj_month_train = month_train - 1\n",
    "# adj_month_val = month_val - 1\n",
    "adj_month_test = month_test - 1\n",
    "\n",
    "# train_dataset = CustomIterableDataset(x_train_model, y_train_model, miss_y_train_model, data_bm[0], adj_month_train, season_train, lulc_data_remapped_int, d_train)\n",
    "# val_dataset = CustomIterableDataset(x_val_model, y_val_model, miss_y_val_model, data_bm[0], adj_month_val, season_val, lulc_data_remapped_int, d_val)\n",
    "test_dataset = CustomIterableDataset(x_test_model, y_test_model, miss_y_test_model, data_bm[0], adj_month_test, season_test, lulc_data_remapped_int, d_test)\n",
    "print(\"Training and validation datasets prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e483605-9623-49f2-9a2c-52ec5843f743",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63464970-3530-4bb5-8b8a-e63ad94ba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hint = 0.2\n",
    "####################################\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "alpha = 0.05\n",
    "beta = 0.05\n",
    "activation_fn = get_activation_function('LeakyReLU')\n",
    "\n",
    "optimizer_name_G = \"AdamW\"\n",
    "optimizer_name_D = \"SGD\"\n",
    "learning_rate_G = 0.00001\n",
    "learning_rate_D = 0.00001\n",
    "g_weight_decay = 1e-5\n",
    "d_weight_decay = 1e-5\n",
    "betas = [0.5, 0.999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b679d6d-19be-49e7-a9a3-33f10503d9fe",
   "metadata": {},
   "source": [
    "## Set file paths for metrics files, models, and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8dc60-16ff-4c6d-9bc6-87cb754ac64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'/datastorage/Output/TrainingData/Pickle/{region_name}/Imputation/'\n",
    "\n",
    "metrics_path = f'/datastorage/Output/Results/Imputation/{region_name}/TVT/'\n",
    "models_path = f'/datastorage/Output/Models/Imputation/{region_name}/'\n",
    "figures_path = f'/datastorage/Output/Figures/Imputation/{region_name}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ff1fb-fa5c-4913-96f4-3a058ce4711a",
   "metadata": {},
   "source": [
    "## Get longs and lats to save the tif fileÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d81ed-bf06-46f8-bf8c-6df98c4411e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "blh_fp = f'/opt/yarn/ml-model/Ancillary_files/Location/{region_name}/blh_2021001.tif'\n",
    "print(blh_fp)\n",
    "with rasterio.open(blh_fp) as src:\n",
    "    blh_data = src.read(1)\n",
    "    transform = src.transform  # Get the affine transformation\n",
    "    width = src.width\n",
    "    height = src.height\n",
    "\n",
    "# Generate a grid of pixel coordinates\n",
    "cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "# Transform pixel coordinates to spatial coordinates\n",
    "xs, ys = rasterio.transform.xy(transform, rows, cols)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "# lats = np.array(ys)\n",
    "# longs = np.array(xs)\n",
    "\n",
    "lats = np.array(ys).reshape(height, width)\n",
    "longs = np.array(xs).reshape(height, width)\n",
    "\n",
    "print(\"Latitude array shape:\", lats.shape)\n",
    "print(\"Longitude array shape:\", longs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0b53c-6e2d-421c-9621-d8be4f06dc0f",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55b64a-6806-4237-a072-50269953a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22286f62-de0b-4577-9fa4-c3d6052c6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W: models/W_discriminator_1121.pth; \n",
    "#NE: models/generator_model_1005_v2.pth models/discriminator_model_1005_v2.pth\n",
    "generator_path = f'{models_path}/generator_model_1005_v2.pth' \n",
    "discriminator_path = f'{models_path}/discriminator_model_1005_v2.pth' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627352b-e3bd-45d8-95a6-923121940fed",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733e7b3-9dde-4ed7-8703-8d5fa3e5b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1. Get the input shape dimensions')\n",
    "\n",
    "# Get the first batch from the DataLoader\n",
    "first_batch = next(iter(test_loader))\n",
    "x_first_batch, y_first_batch = first_batch['X'], first_batch['Y']\n",
    "\n",
    "# Extracting the shape of your input data\n",
    "num_y_segments, in_channels, height, width = y_first_batch.shape\n",
    "num_x_segments, x_channels, height, width = x_first_batch.shape\n",
    "\n",
    "print('2. Set the model')\n",
    "# Instantiate the Generator\n",
    "generator = Generator(in_channels, x_channels, activation_fn, num_lulc_classes)   \n",
    "\n",
    "# Instantiate the Discriminator\n",
    "discriminator = Discriminator(in_channels, activation_fn = activation_fn)\n",
    "\n",
    "print('3. Set the device')\n",
    "use_gpu = torch.cuda.is_available()  # Automatically check if GPU is available\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "torch.manual_seed(42)  # Set seed for CPU operations.\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "print('4. Set optimizer')\n",
    "optimizer_G = get_optimizer(optimizer_name_G, generator.parameters(), learning_rate_G, g_weight_decay, betas, momentum=0.9)\n",
    "optimizer_D = get_optimizer(optimizer_name_D, discriminator.parameters(), learning_rate_D, d_weight_decay, betas, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb7703-df12-4851-bd53-1aea93203889",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce1b21-08fc-4fda-8f11-f45d4b070dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "# Lists to store metrics for each mini-batch across all epochs\n",
    "g_train_loss_history = []\n",
    "d_train_loss_history = []\n",
    "mse_train_loss_history = []\n",
    "mae_train_loss_history = []\n",
    "\n",
    "# Train - Lists to store average metrics per epoch\n",
    "avg_g_train_loss_history = []\n",
    "avg_d_train_loss_history = []\n",
    "avg_mse_train_loss_history = []\n",
    "avg_mae_train_loss_history = []\n",
    "avg_rmse_train_loss_history = []\n",
    "\n",
    "#  Validation - Lists to store average metrics per epoch\n",
    "avg_g_val_loss_history = []\n",
    "avg_d_val_loss_history = []\n",
    "avg_mse_val_loss_history = []\n",
    "avg_mae_val_loss_history = []\n",
    "avg_rmse_val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Initialize variables to track progress\n",
    "    total_g_train_loss = 0\n",
    "    total_d_train_loss = 0\n",
    "    total_mse_train_loss = 0\n",
    "    total_mae_train_loss = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    # train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        print('6. In model training phase')\n",
    "        # Set models to training mode\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "   \n",
    "        # Unpack the batch data\n",
    "        Y_mb = batch_data['Y']  # observed data\n",
    "        X_mb = batch_data['X'] #covariate\n",
    "        M_mb = batch_data['M']  # The mask indicating observed data\n",
    "        binary_mask = batch_data['binary_mask']\n",
    "        month = batch_data['month']\n",
    "        season = batch_data['season']\n",
    "        lulc = batch_data['lulc']\n",
    "        date_tensor = batch_data['date']\n",
    "        \n",
    "        dates = convert_date_tensor_to_datetime(date_tensor)\n",
    "\n",
    "        # First, add a channel dimension to the mask, making it [batch_size, 1, height, width]\n",
    "        binary_mask_expanded = binary_mask.unsqueeze(1) \n",
    "        binary_mask_x_expanded = binary_mask_expanded.expand(-1, in_channels, -1, -1) \n",
    "        binary_mask_y_expanded = binary_mask.unsqueeze(1)\n",
    "        \n",
    "        # Generate random noise Z_mb with the same shape as your data\n",
    "        Z_mb = torch.rand_like(Y_mb) \n",
    "\n",
    "        # Generate hint matrix H_mb1 with a fraction of the observed data points hidden\n",
    "        H_mb1 = sample_M(Y_mb.shape[0], Y_mb.shape[1:], 1 - p_hint)\n",
    "        H_mb = M_mb * H_mb1\n",
    "\n",
    "        # Combine the mask and the random noise to create New_X_mb\n",
    "        New_Y_mb = M_mb * Y_mb + (1 - M_mb) * Z_mb  # Missing Data Introduction\n",
    "\n",
    "        # Apply the ROI mask to ensure operations are only applied within the ROI\n",
    "        New_Y_mb = New_Y_mb * binary_mask_y_expanded\n",
    "        Y_mb = Y_mb * binary_mask_y_expanded\n",
    "        X_mb = X_mb * binary_mask_x_expanded\n",
    "        M_mb = M_mb * binary_mask_y_expanded\n",
    "        H_mb = H_mb * binary_mask_y_expanded\n",
    "\n",
    "        # Transfer to device (if using GPU)\n",
    "        if use_gpu:\n",
    "            Y_mb = Y_mb.to(\"cuda\")\n",
    "            X_mb = X_mb.to(\"cuda\")\n",
    "            M_mb = M_mb.to(\"cuda\")\n",
    "            H_mb = H_mb.to(\"cuda\")\n",
    "            New_Y_mb = New_Y_mb.to(\"cuda\")\n",
    "\n",
    "        # Discriminator - Reset gradients and calculate losses and perform backpropagation\n",
    "        optimizer_D.zero_grad()\n",
    "        D_loss, G_sample = discriminator_loss(generator, discriminator, X_mb, M_mb, New_Y_mb, H_mb, binary_mask_y_expanded, binary_mask_x_expanded, month, season, lulc)\n",
    "        D_loss.backward()\n",
    "\n",
    "        # Zero out gradients for LULC embedding indices 0 and 255\n",
    "        if generator.lulc_embedding.weight.grad is not None:\n",
    "            generator.lulc_embedding.weight.grad[0].fill_(0) \n",
    "            \n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Generator - Reset gradients and calculate losses and perform backpropagation\n",
    "        optimizer_G.zero_grad()\n",
    "        G_loss, Adv_loss, MSE_train_loss, MAE_train_loss, G_sample = generator_loss(generator, discriminator, X_mb, Y_mb, M_mb, New_Y_mb, H_mb, binary_mask_y_expanded, binary_mask_x_expanded, month, season, lulc, alpha, beta)       \n",
    "        G_loss.backward()\n",
    "\n",
    "        # Zero out gradients for LULC embedding indices 0 and 255\n",
    "        if generator.lulc_embedding.weight.grad is not None:\n",
    "            generator.lulc_embedding.weight.grad[0].fill_(0) \n",
    "        \n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Training - Update running totals\n",
    "        total_g_train_loss += G_loss.item()\n",
    "        total_d_train_loss += D_loss.item()\n",
    "        \n",
    "        total_mse_train_loss += MSE_train_loss.item() * Y_mb.size(0) \n",
    "        total_mae_train_loss += MAE_train_loss.item() * Y_mb.size(0)\n",
    "        total_train_samples += Y_mb.size(0)\n",
    "        \n",
    "        # Append the generator and discriminator losses for visualization\n",
    "        g_train_loss_history.append(G_loss.item())\n",
    "        d_train_loss_history.append(D_loss.item())\n",
    "        mse_train_loss_history.append(MSE_train_loss.item())\n",
    "        mae_train_loss_history.append(MAE_train_loss.item())\n",
    "        \n",
    "        iteration += 1\n",
    "\n",
    "        print(f'Epoch {epoch} - batch {i} Generator loss: {G_loss.item():.4f}, D_loss: {D_loss.item():.4f}, Adversarial_train_loss: {Adv_loss.item():.4f}, MSE_train_loss: {MSE_train_loss.item():.4f}, MAE_train_loss: {MAE_train_loss.item():.4f}')\n",
    "\n",
    "    # Calculate average losses for the epoch\n",
    "    avg_g_train_loss = total_g_train_loss / total_train_samples\n",
    "    avg_d_train_loss = total_d_train_loss / total_train_samples\n",
    "    avg_train_mse = total_mse_train_loss / total_train_samples\n",
    "    avg_train_mae = total_mae_train_loss / total_train_samples\n",
    "    avg_train_rmse = np.sqrt(total_mse_train_loss / total_train_samples)\n",
    "    \n",
    "    # At the end of the epoch, append the averages\n",
    "    avg_g_train_loss_history.append(avg_g_train_loss)\n",
    "    avg_d_train_loss_history.append(avg_d_train_loss)\n",
    "    avg_mse_train_loss_history.append(avg_train_mse)\n",
    "    avg_mae_train_loss_history.append(avg_train_mae)\n",
    "    avg_rmse_train_loss_history.append(avg_train_rmse)\n",
    "\n",
    "    print(f'Epoch {epoch} | Average train G_loss: {avg_g_train_loss:.4f}, D_loss: {avg_d_train_loss:.4f}, MSE_train: {avg_train_mse:.4f}, MAE_train: {avg_train_mae:.4f}, RMSE_train: {avg_train_rmse:.4f}')\n",
    "\n",
    "    print('===========================================================')\n",
    "    print('7. In model validation phase')\n",
    "    # Switch to evaluation mode for validation\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    total_g_val_loss = 0\n",
    "    total_d_val_loss = 0\n",
    "    total_mse_val_loss = 0\n",
    "    total_mae_val_loss = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for i, batch_data in enumerate(val_loader):\n",
    "            print(f'epoch {epoch} batch data {i}')\n",
    "            X_mb = batch_data['X'].to(device)\n",
    "            M_mb = batch_data['M'].to(device)\n",
    "            Y_mb = batch_data['Y'].to(device)\n",
    "            binary_mask = batch_data['binary_mask']\n",
    "            month = batch_data['month']\n",
    "            season = batch_data['season']\n",
    "            lulc = batch_data['lulc']\n",
    "\n",
    "            # First, add a channel dimension to the mask, making it [batch_size, 1, height, width]\n",
    "            binary_mask_expanded = binary_mask.unsqueeze(1) \n",
    "            binary_mask_x_expanded = binary_mask_expanded.expand(-1, x_channels, -1, -1) \n",
    "            binary_mask_y_expanded = binary_mask.unsqueeze(1)\n",
    "\n",
    "            # Generate random noise Z_mb with the same shape as your data\n",
    "            Z_mb = torch.rand_like(Y_mb)\n",
    "\n",
    "            # Generate hint matrix H_mb1 with a fraction of the observed data points hidden\n",
    "            H_mb1 = sample_M(Y_mb.shape[0], Y_mb.shape[1:], 1 - p_hint)\n",
    "            H_mb = M_mb * H_mb1\n",
    "\n",
    "            # Combine the mask and the random noise to create New_X_mb\n",
    "            New_Y_mb = M_mb * Y_mb + (1 - M_mb) * Z_mb  # Missing Data Introduction\n",
    "\n",
    "            # Apply the ROI mask to ensure operations are only applied within the ROI\n",
    "            New_Y_mb = New_Y_mb * binary_mask_y_expanded\n",
    "            X_mb = X_mb * binary_mask_x_expanded\n",
    "            M_mb = M_mb * binary_mask_y_expanded\n",
    "            H_mb = H_mb * binary_mask_y_expanded\n",
    "\n",
    "            # Discriminator loss\n",
    "            D_loss, G_sample = discriminator_loss(generator, discriminator, X_mb, M_mb, New_Y_mb, H_mb, binary_mask_y_expanded, binary_mask_x_expanded, month, season, lulc)\n",
    "            # Generator loss\n",
    "            G_loss, Adv_loss, MSE_val_loss, MAE_val_loss, G_sample = generator_loss(generator, discriminator, X_mb, Y_mb, M_mb, New_Y_mb, H_mb, binary_mask_y_expanded, binary_mask_x_expanded, month, season, lulc, alpha, beta)\n",
    "\n",
    "            # Validation - Update running totals\n",
    "            total_g_val_loss += G_loss.item()\n",
    "            total_d_val_loss += D_loss.item()\n",
    "            total_mse_val_loss += MSE_val_loss.item() * Y_mb.size(0)  \n",
    "            total_mae_val_loss += MAE_val_loss.item() * Y_mb.size(0)\n",
    "            total_val_samples += Y_mb.size(0)\n",
    "            \n",
    "        avg_g_val_loss = total_g_val_loss / total_val_samples\n",
    "        avg_d_val_loss = total_d_val_loss / total_val_samples\n",
    "        avg_val_mse = total_mse_val_loss / total_val_samples\n",
    "        avg_val_mae = total_mae_val_loss / total_val_samples\n",
    "        avg_val_rmse = np.sqrt(total_mse_val_loss / total_val_samples) \n",
    "        avg_val_mae = total_mae_val_loss / total_val_samples\n",
    "        \n",
    "        # At the end of the epoch, append the averages\n",
    "        avg_g_val_loss_history.append(avg_g_val_loss)\n",
    "        avg_d_val_loss_history.append(avg_d_val_loss)\n",
    "        avg_mse_val_loss_history.append(avg_val_mse)\n",
    "        avg_mae_val_loss_history.append(avg_val_mae)\n",
    "        avg_rmse_val_loss_history.append(avg_val_rmse)\n",
    "\n",
    "        print(f'Epoch {epoch} | Average val G_loss: {avg_g_val_loss:.4f}, D_loss: {avg_d_val_loss:.4f},MSE_val: {avg_val_mse:.4f}, MAE_val: {avg_val_mae:.4f}, RMSE_val: {avg_val_rmse:.4f}')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae1a7a-6826-4e71-8165-533eac7f7956",
   "metadata": {},
   "source": [
    "## Run this code with caution - DON'T RUN ME WITHOUT THINKING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322d3ae-2f60-4a43-ae8d-feadc0be8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generator model\n",
    "torch.save(generator.state_dict(), generator_path)\n",
    "\n",
    "# Save the discriminator model\n",
    "torch.save(discriminator.state_dict(), discriminator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49bf1b-7623-4974-828a-dde2326170df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to form DataFrame columns\n",
    "data1 = {\n",
    "    \"Train MSE\": avg_mse_train_loss_history,\n",
    "    \"Val MSE\": avg_mse_val_loss_history\n",
    "}\n",
    "\n",
    "# Create a dictionary to form DataFrame columns\n",
    "data2 = {\n",
    "    \"Train MAE\": avg_mae_train_loss_history,\n",
    "    \"Val MAE\": avg_mae_val_loss_history\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Specify the file name\n",
    "csv_file1 = f\"{metrics_path}{region_abbr}_LC_MSE.csv\"\n",
    "csv_file2 = f\"{metrics_path}{region_abbr}_LC_MAE.csv\"\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df1.to_csv(csv_file1, index=False)\n",
    "df2.to_csv(csv_file2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e72b9e-4e02-42cd-a63b-1ca493f0b279",
   "metadata": {},
   "source": [
    "## Learning curve - train and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc1a0b-dc6d-4d14-89d9-58140f2578f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs corresponds to the length of the lists\n",
    "epochs = range(1, len(avg_mse_train_loss_history) + 1)\n",
    "\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plotting the MSE learning curve\n",
    "axs[0].plot(epochs, avg_mse_train_loss_history, 'b-', marker='o', label='Training MSE')\n",
    "axs[0].plot(epochs, avg_mse_val_loss_history, 'r-', marker='o', label='Validation MSE')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('MSE')\n",
    "axs[0].set_title('MSE over Epochs')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plotting the MAE learning curve\n",
    "axs[1].plot(epochs, avg_mae_train_loss_history, 'b-', marker='o', label='Training MAE')\n",
    "axs[1].plot(epochs, avg_mae_val_loss_history, 'r-', marker='o', label='Validation MAE')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('MAE')\n",
    "axs[1].set_title('MAE over Epochs')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9145b2-3dde-45c2-a05e-687aaea9cf8c",
   "metadata": {},
   "source": [
    "## Loads the saved model for evaluation and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff677ce-01b0-4012-aeb2-1ca13fae79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_state_dict(torch.load(generator_path, map_location=torch.device('cpu')))\n",
    "# discriminator.load_state_dict(torch.load(discriminator_path, map_location=torch.device('cpu')))\n",
    "\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=\"cpu\", weights_only=True))\n",
    "discriminator.load_state_dict(torch.load(discriminator_path, map_location=\"cpu\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230de96-a8c5-4618-9235-9e8f5443b71a",
   "metadata": {},
   "source": [
    "## Call the evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7fbf6-6b51-4ad8-9417-036cac713086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputed_train_data_samples, gain_train_data_samples, y_train_samples, train_date_list, train_y_mask_list, train_metrics_df = eval(train_loader, generator, discriminator, p_hint, alpha, beta, device)\n",
    "# imputed_val_data_samples, gain_val_data_samples, y_val_samples, val_date_list, val_y_mask_list, val_metrics_df = eval(val_loader, generator, discriminator, p_hint, alpha, beta, device)\n",
    "imputed_test_data_samples, gain_test_data_samples, y_test_samples, test_date_list, test_y_mask_list, test_metrics_df = eval(test_loader, generator, discriminator, p_hint, alpha, beta, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9bbd40-d9fa-4ad1-8b4d-8da3b144af20",
   "metadata": {},
   "source": [
    "## Save train validation and test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df21d15-dc52-4fa3-a850-65a84c281773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_df.to_csv(f'{region_abbr}_train_metrics.csv')\n",
    "val_metrics_df.to_csv(f'{region_abbr}_val_metrics.csv')\n",
    "test_metrics_df.to_csv(f'{region_abbr}_test_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361e6a3-f890-4637-8811-b2ccb5d97870",
   "metadata": {},
   "source": [
    "## Process imputed data - sort dates and mask the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b58232-3d7e-4c7c-85fe-8f39da4e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputed_train_aod, gain_train_imputed_aod, y_train_data, sorted_train_dates = process_imputed_data(imputed_train_data_samples, gain_train_data_samples, miss_y_train_model, y_train_samples, train_date_list, data_bm[0])\n",
    "# imputed_val_aod, gain_val_imputed_aod, y_val_data, sorted_val_dates = process_imputed_data(imputed_val_data_samples, gain_val_data_samples, miss_y_val_model, y_val_samples, val_date_list, data_bm[0])\n",
    "imputed_test_aod, gain_test_imputed_aod, y_test_data, sorted_test_dates = process_imputed_data(imputed_test_data_samples, gain_test_data_samples, miss_y_test_model, y_test_samples, test_date_list, data_bm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6af74-5bc9-448e-9b51-4adf8122a7f6",
   "metadata": {},
   "source": [
    "## Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41726cb0-c37a-4d26-b471-54cbb909573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime(2022, 8, 8, 0, 0)  # Replace with the desired date\n",
    "# create_plots(imputed_train_aod, gain_train_imputed_aod, y_train_data, miss_y_train_model, sorted_train_dates, date, 'rainbow')\n",
    "# create_plots(imputed_val_aod, gain_val_imputed_aod, y_val_data, miss_y_val_model, sorted_val_dates, date, 'rainbow')\n",
    "create_plots(imputed_test_aod, gain_test_imputed_aod, y_test_data, miss_y_test_model, sorted_test_dates, date, 'rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc43c2-3527-4bcd-891c-23709c4811dd",
   "metadata": {},
   "source": [
    "## Save imputed tif data for train test and validation\n",
    "* create xarray dataset\n",
    "* save the xarray dataset as raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54ab05-58b6-4caa-93b5-fbc4de0d6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(f'/data/Imputation/{region_name}/Output/Version1/TIF/GAIN/NonRT/'):\n",
    "    os.makedirs(f'/data/Imputation/{region_name}/Output/Version1/TIF/GAIN/NonRT/')\n",
    "train_imputed_xr = transform_and_create_dataset(gain_train_imputed_aod, d_train, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/NonRT/', train_imputed_xr)\n",
    "val_imputed_xr = transform_and_create_dataset(gain_val_imputed_aod, d_val, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/NonRT/', val_imputed_xr)\n",
    "test_imputed_xr = transform_and_create_dataset(gain_test_imputed_aod, d_test, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/NonRT/', test_imputed_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6219786-e3fc-4fd3-a07b-0c38243ca832",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'/data/Imputation/{region_name}/Output/Version1/TIF/Merged/NonRT/'):\n",
    "    os.makedirs(f'/data/Imputation/{region_name}/Output/Version1/TIF/Merged/NonRT/')\n",
    "train_imputed_xr = transform_and_create_dataset(imputed_train_aod, d_train, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/Merged/NonRT/', train_imputed_xr)\n",
    "val_imputed_xr = transform_and_create_dataset(imputed_val_aod, d_val, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/Merged/NonRT/', val_imputed_xr)\n",
    "test_imputed_xr = transform_and_create_dataset(imputed_test_aod, d_test, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/Merged/NonRT/', test_imputed_xr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad8b20-72ae-4909-a674-9d80d69f3c3a",
   "metadata": {},
   "source": [
    "## Evaluate with UQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1a89e-692d-4573-b7fd-043707286bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_gain_data_samples, uq_variance_data_samples,  lower_bound_samples, upper_bound_samples, uq_y_actual_samples, uq_y_mask_list, uq_date_list = eval_with_mcd(test_loader, generator, discriminator, p_hint, alpha, beta, z_score99, device, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8af46-904f-4b5a-baf6-e35278b9bed3",
   "metadata": {},
   "source": [
    "## Process imputed data with UQ - sort dates and mask the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5a53b-c9d6-4506-a378-b5be4fd28447",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_imputed_test_aod, var_test_aod, lower_bound_test_aod, upper_bound_test_aod, actual_test_aod, sorted_test_dates = process_imputed_data_uq(uq_gain_data_samples, uq_variance_data_samples, lower_bound_samples, upper_bound_samples, uq_y_actual_samples, miss_y_test_model, uq_date_list, data_bm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd9c56-1fba-49b2-9948-6dc8cc304857",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_width = upper_bound_test_aod - lower_bound_test_aod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7887dde4-8008-431e-8da5-2d5cd5a3bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_width_mean = pi_width.mean(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04aedfe-37e1-40fc-a77d-4cb88788a9c8",
   "metadata": {},
   "source": [
    "## Save uncertainty quantified data as tif data for test data\n",
    "* create xarray dataset\n",
    "* save the xarray dataset as rasterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eefcdc-9826-4671-b09c-b11e4c0eca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(f'{tif_path}/GAIN/NonRT/'):\n",
    "    os.makedirs(f'{tif_path}/GAIN/NonRT/')\n",
    "train_imputed_xr = transform_and_create_dataset(gain_train_imputed_aod, d_train, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/NonRT/', train_imputed_xr)\n",
    "val_imputed_xr = transform_and_create_dataset(gain_val_imputed_aod, d_val, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/NonRT/', val_imputed_xr)\n",
    "test_imputed_xr = transform_and_create_dataset(gain_test_imputed_aod, d_test, longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/NonRT/', test_imputed_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a347ef3-aa27-495b-a469-b32319a76c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_pkl}/{region_abbr}_X_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    x_data_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9550619-cbb7-4f8b-8db5-57c6f84c55d0",
   "metadata": {},
   "source": [
    "## Read the inference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890e38b-031e-4018-9342-c184175a53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_year = 2024\n",
    "# Load X and Y datasets\n",
    "with open(f'{output_pkl}/{region_abbr}_X_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    x_data_model = pickle.load(f)\n",
    "\n",
    "# Load X and Y datasets\n",
    "with open(f'{output_pkl}/{region_abbr}_Y_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "   y_data_model, miss_matrix_model = pickle.load(f)\n",
    "\n",
    "# Load date arrays\n",
    "with open(f'{output_pkl}/{region_abbr}_DATE_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    date_arr = pickle.load(f)\n",
    "\n",
    "# Load binary mask\n",
    "with open(f'{output_pkl}/{region_abbr}_BM_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    data_bm = pickle.load(f)\n",
    "\n",
    "# Load grid information (latitude and longitude)\n",
    "with open(f'{output_pkl}/{region_abbr}_Grid_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    latitude, longitude = pickle.load(f)\n",
    "\n",
    "# Load season array\n",
    "with open(f'{output_pkl}/{region_abbr}_Season_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    season_arr = pickle.load(f)\n",
    "\n",
    "# Load month array\n",
    "with open(f'{output_pkl}/{region_abbr}_Month_Infer_{infer_year}.pkl', 'rb') as f:\n",
    "    month_arr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30957a8c-b54f-43eb-8747-ce47a9b972d6",
   "metadata": {},
   "source": [
    "## Create infer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631aa784-6b19-47cf-8929-58f064f5bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = data_bm[0]\n",
    "adj_month_arr = month_arr - 1\n",
    "infer_dataset = CustomIterableDataset(x_data_model[0], y_data_model, miss_matrix_model, bm, adj_month_arr, season_arr, lulc_data_remapped, date_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c59ce5-6908-4047-b58c-1b5baf457031",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_loader = DataLoader(infer_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596750b-42f8-4799-9bb5-247b46a0cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch from the DataLoader\n",
    "first_batch = next(iter(infer_loader))\n",
    "x_first_batch, y_first_batch = first_batch['X'], first_batch['Y']\n",
    "\n",
    "# Extracting the shape of your input data\n",
    "num_y_segments, in_channels, height, width = y_first_batch.shape\n",
    "num_x_segments, x_channels, height, width = x_first_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6841f-7337-4367-ac5c-2d382f62b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Generator and Discriminator\n",
    "generator = Generator(in_channels, x_channels, activation_fn, num_lulc_classes)\n",
    "discriminator = Discriminator(in_channels, activation_fn=activation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0d3c2-900d-4f89-a4aa-c4bfbcf5ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights into the model - make sure to map the location to CPU if you're not using GPU for inference\n",
    "generator.load_state_dict(torch.load(generator_path, map_location=torch.device('cpu')))\n",
    "discriminator.load_state_dict(torch.load(discriminator_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode before inference\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8358d11-5c6c-44e3-a714-f5b74c635ebd",
   "metadata": {},
   "source": [
    "## Run the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970762a-556e-4375-894d-c12755a25e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_infer_data_samples, gain_infer_data_samples, y_infer_samples, infer_date_list, infer_y_mask_list,all_metrics_df = run_inference(infer_loader, generator, discriminator, p_hint, alpha, beta, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d55af4-ceac-4252-ae2f-22fed8a56d62",
   "metadata": {},
   "source": [
    "## Save inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640a3d9-63e7-4e77-8395-4fe6d853be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_df.to_csv(f'{metrics_path}/{region_abbr}_infer_metrics_2024.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72b312-e2ad-4590-b3a6-05f9a18dd48e",
   "metadata": {},
   "source": [
    "## Process imputed data - sort dates and mask the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411bf8a-431c-4db6-8b8f-c86e1ef78769",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_infer_aod, gain_infer_imputed_aod, y_infer_data, sorted_infer_dates = process_imputed_data(imputed_infer_data_samples, gain_infer_data_samples, miss_matrix_model, y_infer_samples, infer_date_list, data_bm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289e65c-6791-43fb-aac1-51c5ef058be9",
   "metadata": {},
   "source": [
    "## Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ff05d-b020-4880-890c-cb84d8290d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime(2024, 2, 24, 0, 0)  # Replace with the desired date\n",
    "create_plots(imputed_infer_aod, gain_infer_imputed_aod, y_infer_data, miss_matrix_model, sorted_infer_dates, date, 'rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce8efa-6061-4c06-a0b2-a13565fc4ba3",
   "metadata": {},
   "source": [
    "## Save imputed inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303db368-1b85-48c0-b7e1-c1dfe2a4a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_path, infer_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2841e2f-134a-404a-a54a-b82e657d5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_imputed_xr = create_xr(gain_infer_imputed_aod, date_arr[0], longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/GAIN/{infer_year}/', infer_imputed_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db31b7c-8e3e-4911-a504-151e289a3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_imputed_xr = create_xr(imputed_infer_aod, date_arr[0], longs, lats, proj_string)\n",
    "save_output(f'{tif_path}/Merged/{infer_year}/', infer_imputed_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506587a3-4bc7-4cf4-80a7-50775effe23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils import sample_M\n",
    "def enable_dropout(model):\n",
    "    \"\"\"Function to enable dropout layers during evaluation.\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (torch.nn.Dropout, torch.nn.Dropout2d)):\n",
    "            module.train()  \n",
    "\n",
    "def eval_with_mcd(loader, generator, discriminator, p_hint, alpha, beta, z_score, device, num_samples=10):\n",
    "    generator.eval()  # Set the generator to evaluation mode\n",
    "    discriminator.eval()  # Set the discriminator to evaluation mode\n",
    "    \n",
    "    # Enable dropout layers during evaluation\n",
    "    enable_dropout(generator)\n",
    "\n",
    "    gain_data_samples = []\n",
    "    variance_data_samples = []\n",
    "    lower_bound_samples = []\n",
    "    upper_bound_samples = []\n",
    "    y_actual_samples = []\n",
    "    date_list = []\n",
    "    y_mask_list = []\n",
    "\n",
    "    # all_metrics_df = pd.DataFrame()\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for batch_data in loader:\n",
    "            \n",
    "            X_mb = batch_data['X'].to(device)\n",
    "            M_mb = batch_data['M'].to(device)\n",
    "            Y_mb = batch_data['Y'].to(device)\n",
    "            binary_mask = batch_data['binary_mask']\n",
    "            month = batch_data['month']\n",
    "            season = batch_data['season']\n",
    "            lulc = batch_data['lulc']\n",
    "            date_tensor = batch_data['date']\n",
    "\n",
    "            dates = convert_tr_dt_to_dt(date_tensor)\n",
    "\n",
    "            num_x_segments, x_channels, height, width = X_mb.shape\n",
    "            num_y_segments, in_channels, height, width = Y_mb.shape\n",
    "\n",
    "            # First, add a channel dimension to the mask, making it [batch_size, 1, height, width]\n",
    "            binary_mask_expanded = binary_mask.unsqueeze(1) \n",
    "            binary_mask_x_expanded = binary_mask_expanded.expand(-1, x_channels, -1, -1) \n",
    "            binary_mask_y_expanded = binary_mask.unsqueeze(1)\n",
    "\n",
    "            # Generate random noise Z_mb with the same shape as your data\n",
    "            Z_mb = torch.rand_like(Y_mb)\n",
    "\n",
    "            # Generate hint matrix H_mb1 with a fraction of the observed data points hidden\n",
    "            H_mb1 = sample_M(Y_mb.shape[0], Y_mb.shape[1:], 1 - p_hint)\n",
    "            H_mb = M_mb * H_mb1\n",
    "\n",
    "            # Combine the mask and the random noise to create New_X_mb\n",
    "            New_Y_mb = M_mb * Y_mb + (1 - M_mb) * Z_mb  # Missing Data Introduction\n",
    "\n",
    "            # Apply the ROI mask to ensure operations are only applied within the ROI\n",
    "            New_Y_mb = New_Y_mb * binary_mask_y_expanded\n",
    "            X_mb = X_mb * binary_mask_x_expanded\n",
    "            M_mb = M_mb * binary_mask_y_expanded\n",
    "            H_mb = H_mb * binary_mask_y_expanded\n",
    "\n",
    "            # Verify if dropout layers are in training mode\n",
    "            for m in generator.modules():\n",
    "                if isinstance(m, torch.nn.Dropout) or isinstance(m, torch.nn.Dropout2d):\n",
    "                    print(f\"Dropout layer {m} in training mode: {m.training}\")\n",
    "\n",
    "            # Perform multiple stochastic forward passes to capture uncertainty\n",
    "            all_imputed_samples = []\n",
    "            for i in range(num_samples):\n",
    "                print(f'running the sample {i}')\n",
    "                Imputed_Y_mb = generator(New_Y_mb * binary_mask_y_expanded, \n",
    "                                         M_mb * binary_mask_y_expanded, \n",
    "                                         X_mb * binary_mask_x_expanded, \n",
    "                                         month, season, lulc, binary_mask_y_expanded).detach()\n",
    "                Imputed_Y_mb *= binary_mask_y_expanded  # Mask out outside ROI\n",
    "                all_imputed_samples.append(Imputed_Y_mb)\n",
    "\n",
    "            # Stack the predictions using torch.stack\n",
    "            all_imputed_samples = torch.stack(all_imputed_samples, dim=0)\n",
    "            binary_mask_y_expanded = binary_mask_y_expanded.unsqueeze(0).expand(all_imputed_samples.shape)\n",
    "            \n",
    "            nan_mask = ~binary_mask_y_expanded.bool()  # Identify outside-ROI pixels\n",
    "            all_imputed_samples[nan_mask] = float('nan')\n",
    "\n",
    "            # Optional way\n",
    "            # Calculate the mean and variance along the first dimension (stochastic passes)\n",
    "            # mean_imputed_Y_mb = torch.nanmean(all_imputed_samples, dim=0)\n",
    "            # variance_imputed_Y_mb = torch.nanvar(all_imputed_samples, dim=0)\n",
    "            # std_imputed_Y_mb = torch.nanstd(all_imputed_samples, dim=0)\n",
    "            # lower_bound = mean_imputed_Y_mb - z_score * std_imputed_Y_mb\n",
    "            # upper_bound = mean_imputed_Y_mb + z_score * std_imputed_Y_mb\n",
    "\n",
    "            valid_mask = binary_mask_y_expanded.bool() \n",
    "            mean_imputed_Y_mb = torch.sum(all_imputed_samples * valid_mask, dim=0) / valid_mask.sum(dim=0)\n",
    "            variance_imputed_Y_mb = torch.sum(((all_imputed_samples - mean_imputed_Y_mb) ** 2) * valid_mask, dim=0) / valid_mask.sum(dim=0)\n",
    "            std_imputed_Y_mb = torch.sqrt(\n",
    "                torch.sum(((all_imputed_samples - mean_imputed_Y_mb) ** 2) * valid_mask, dim=0) / valid_mask.sum(dim=0)\n",
    "            )\n",
    "            lower_bound = mean_imputed_Y_mb - z_score * std_imputed_Y_mb\n",
    "            upper_bound = mean_imputed_Y_mb + z_score * std_imputed_Y_mb\n",
    "\n",
    "            # Compute single scalar values\n",
    "            mean_value = torch.nanmean(mean_imputed_Y_mb).item()\n",
    "            var_value = torch.nanmean(variance_imputed_Y_mb).item()\n",
    "            std_value = torch.nanmean(std_imputed_Y_mb).item() \n",
    "            # Compute single scalar values while ignoring NaNs\n",
    "            lb_value = torch.nanmean(lower_bound).item()\n",
    "            ub_value = torch.nanmean(upper_bound).item()\n",
    "            \n",
    "            # Print the values\n",
    "            print(f\"Mean: {mean_value}, Variance: {var_value}, Standard deviation: {std_value}, lower bound: {lb_value}, upper bound: {ub_value}\")\n",
    "\n",
    "            # Integrate imputed and observed data\n",
    "            # Final_Imputed_Y_mb = M_mb * Y_mb + (1 - M_mb) * mean_imputed_Y_mb\n",
    "            # Final_Imputed_Y_mb *= binary_mask_y_expanded  # Mask out pixels outside the ROI\n",
    "            # mean_imputed_Y_mb *= binary_mask_y_expanded \n",
    "            # variance_imputed_Y_mb *= binary_mask_y_expanded\n",
    "            # std_imputed_Y_mb *= binary_mask_y_expanded\n",
    "    \n",
    "            # Store the mean imputed values and uncertainty\n",
    "            # imputed_data_samples.append(Final_Imputed_Y_mb.cpu().numpy()) \n",
    "            gain_data_samples.append(mean_imputed_Y_mb.cpu().numpy())\n",
    "            variance_data_samples.append(variance_imputed_Y_mb.cpu().numpy())\n",
    "            lower_bound_samples.append(lower_bound.cpu().numpy())\n",
    "            upper_bound_samples.append(upper_bound.cpu().numpy())\n",
    "            y_actual_samples.append(Y_mb.cpu().numpy())\n",
    "            y_mask_list.append(M_mb)\n",
    "            date_list.extend(dates)\n",
    "\n",
    "            # Optionally, calculate any metrics (like RMSE) using the mean imputed values\n",
    "            # batch_metrics_df = calc_metrics(dates, M_mb,mean_imputed_Y_mb, Y_mb.cpu())\n",
    "            # all_metrics_df = pd.concat([all_metrics_df, batch_metrics_df], ignore_index=True)\n",
    "\n",
    "            # total_test_samples += Y_mb.size(0)\n",
    "    \n",
    "    return gain_data_samples, variance_data_samples, lower_bound_samples, upper_bound_samples, y_actual_samples, y_mask_list, date_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e383ac-68be-4a33-9fb1-126c1be88089",
   "metadata": {},
   "source": [
    "## Uncertainty quantification using MCD\n",
    "* Evaluation with MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123839da-5390-4dd3-a68b-6d9bda2e79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_gain_infer_samples, uq_variance_infer_samples, lower_bound_infer_samples, upper_bound_infer_samples, uq_y_actual_infer_samples, uq_y_mask_infer_list, uq_date_infer_list  = eval_with_mcd(infer_loader, generator, discriminator, p_hint, alpha, beta, z_score99, device, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451c5e3-f70a-4ff3-9a73-1d5d8b44db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_imputed_data_uq(gain_data_samples, variance_data_samples, lower_data_samples, upper_data_samples, y_actual_samples, miss_data, all_dates, bm):\n",
    "    \n",
    "    gain_concat = np.concatenate(gain_data_samples, axis=0)\n",
    "    var_concat = np.concatenate(variance_data_samples, axis=0)\n",
    "    lower_b_concat = np.concatenate(lower_data_samples, axis=0)\n",
    "    upper_b_concat = np.concatenate(upper_data_samples, axis=0)\n",
    "    y_concat = np.concatenate(y_actual_samples, axis = 0)\n",
    "\n",
    "    total_days, channels, height, width = var_concat.shape\n",
    "\n",
    "    combined = list(zip(all_dates, gain_concat, var_concat, lower_b_concat, upper_b_concat, y_concat))\n",
    "    combined.sort(key=lambda x: x[0])\n",
    "    sorted_dates, sorted_gain_concat, sorted_var_concat, sorted_lower_data, sorted_upper_data, sorted_y_data = zip(*combined)\n",
    "\n",
    "    sorted_gain_concat = np.array(sorted_gain_concat)\n",
    "    sorted_var_concat = np.array(sorted_var_concat)\n",
    "    sorted_lower_data = np.array(sorted_lower_data)\n",
    "    sorted_upper_data = np.array(sorted_upper_data)\n",
    "    sorted_y_data = np.array(sorted_y_data)\n",
    "\n",
    "    expanded_bm = np.expand_dims(np.expand_dims(bm, axis=0), axis=0)\n",
    "    expanded_bm = np.repeat(expanded_bm, total_days, axis=0)\n",
    "    \n",
    "    gain_imputed_aod = np.where(expanded_bm == 0, np.nan, sorted_gain_concat)\n",
    "    var_aod = np.where(expanded_bm == 0, np.nan, sorted_var_concat)\n",
    "    lower_bound_aod = np.where(expanded_bm == 0, np.nan, sorted_lower_data)\n",
    "    upper_bound_aod = np.where(expanded_bm == 0, np.nan, sorted_upper_data)\n",
    "    actual_aod = np.where((expanded_bm == 0)|(miss_data==0), np.nan, sorted_y_data)\n",
    "    \n",
    "    return gain_imputed_aod, var_aod, lower_bound_aod, upper_bound_aod, actual_aod, sorted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411e47b-fe5d-4ac9-9102-de81c2ba3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_imputed_infer_aod, var_infer_aod, lower_bound_infer_aod, upper_bound_infer_aod, actual_infer_aod, sorted_infer_dates = process_imputed_data_uq(uq_gain_infer_samples, uq_variance_infer_samples, lower_bound_infer_samples, upper_bound_samples, uq_y_actual_infer_samples, miss_matrix_model, uq_date_infer_list, data_bm[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0c686-2e1a-4da1-af22-4ea3764c98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_xr(imputed_aod, date_arr, longitude, latitude, proj_string):\n",
    "    x_prj = longitude[0,:]\n",
    "    y_prj = latitude[:,0]\n",
    "    date_xr = np.array(date_arr, dtype='datetime64[ns]')\n",
    "    imputed_data = xr.Dataset(\n",
    "        {\n",
    "            \"data\": ([\"time\", \"latitude\", \"longitude\"], imputed_aod[:,0,:])\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": (\"time\", date_xr),\n",
    "            \"longitude\": (\"longitude\", x_prj),\n",
    "            \"latitude\": (\"latitude\", y_prj),\n",
    "        }\n",
    "    )\n",
    "    imputed_data.rio.write_crs(proj_string, inplace=True)\n",
    "    return imputed_data\n",
    "    \n",
    "def save_output(output_path, file_substr, imputed_data, save_mode = 1):\n",
    "    print(type(imputed_data))\n",
    "    if save_mode == 1:\n",
    "        for time_idx in range(len(imputed_data.time)):\n",
    "            # Select the data at this particular time\n",
    "            single_time_slice = imputed_data.isel(time=time_idx)\n",
    "            timestamp = pd.to_datetime(single_time_slice.time.values)\n",
    "            year_doy = timestamp.strftime('%Y%j')  \n",
    "            print(f'Saving {output_path}MAIAC_{year_doy}_{file_substr}.tif')\n",
    "            tif_filename = f\"{output_path}MAIAC_{year_doy}_{file_substr}.tif\"\n",
    "            single_time_slice.rio.to_raster(tif_filename)\n",
    "\n",
    "    elif save_mode == 2:\n",
    "        date_arr = imputed_data.time.values\n",
    "        pd_date = pd.Timestamp(date_arr[0])\n",
    "        year = pd_date.year\n",
    "        nc4_filename = f\"output_path{output_nc4}MAIAC_{year}_GAIN.nc\"\n",
    "        imputed_data.to_netcdf(path=output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58237c-a97c-4d97-823d-7e2bf8bf1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "uq_path = '/data/Imputation/Northeast/Output/Version3/UQ/'\n",
    "if not os.path.exists(f'{uq_path}/'):\n",
    "    os.makedirs(f'{uq_path}/')\n",
    "var_infer_xr = create_xr(var_infer_aod, date_arr[0], longs, lats, proj_string)\n",
    "save_output(f'{uq_path}/', 'GAIN_UQ', var_infer_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1fd89-6c5a-464d-9db0-6849e8b29c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_width_infer = upper_bound_infer_aod - lower_bound_infer_aod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb6dc4-129d-4a53-ba22-a67dbc7dee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 136\n",
    "print(np.nanmin(pi_width_infer[idx, 0, :]))\n",
    "print(np.nanmax(pi_width_infer[idx, 0, :]))\n",
    "plt.imshow(pi_width_infer[idx,0,:], cmap='rainbow')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db55be-d2f5-4d51-8d95-08428df04c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_path = '/data/Imputation/Northeast/Output/Version3/CI/'\n",
    "ci_infer_xr = create_xr(pi_width_infer, date_arr[0], longs, lats, proj_string)\n",
    "save_output(f'{ci_path}/', 'GAIN_CI', ci_infer_xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef4efce-9c4c-4abf-a98b-04c360176a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 156\n",
    "\n",
    "print(np.nanmin(test_concat[idx,0,:]), np.nanmax(test_concat[idx,0,:]))\n",
    "print(np.nanmin(y_data[idx,0,:]), np.nanmax(y_data[idx,0,:]))\n",
    "print(np.nanmin(y_data_model[idx,0,:]), np.nanmax(y_data_model[idx,0,:]))\n",
    "max_val = np.nanmax(y_data[idx, 0, :])\n",
    "min_val = np.nanmin(y_data[idx, 0, :])\n",
    "\n",
    "# Create subplots with a single row and three columns\n",
    "fig, axs = plt.subplots(1, 4, figsize=(18, 6))\n",
    "\n",
    "# Define the color map and norm\n",
    "cmap = 'jet'\n",
    "norm = plt.Normalize(min_val, max_val)\n",
    "\n",
    "# Plot infer_concat\n",
    "cax1 = axs[0].imshow(test_concat[idx, 0, :], cmap=cmap, norm = norm)\n",
    "axs[0].set_title('Infer Concat')\n",
    "fig.colorbar(cax1, ax=axs[0], extend='both')\n",
    "\n",
    "# Plot y_data\n",
    "cax2 = axs[1].imshow(y_data[idx, 0, :], cmap=cmap, norm = norm)\n",
    "axs[1].set_title('Y Data')\n",
    "fig.colorbar(cax2, ax=axs[1], extend='both')\n",
    "\n",
    "# Plot infer_concat1\n",
    "cax3 = axs[2].imshow(y_data_model[idx, 0, :], cmap = cmap, norm = norm)\n",
    "axs[2].set_title('Infer Concat 1')\n",
    "fig.colorbar(cax3, ax=axs[2], extend='both')\n",
    "\n",
    "# Plot infer_concat1\n",
    "cax4 = axs[3].imshow(miss_matrix_model[idx, 0, :], cmap = cmap)\n",
    "axs[3].set_title('Infer Concat 1')\n",
    "fig.colorbar(cax4, ax=axs[3], extend='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
