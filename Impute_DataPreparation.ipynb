{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a376a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "import rasterio \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time, os \n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20921a3a-dcc0-4fd5-874b-9999fec496f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_dict_region = {'Northeast': 'NE',\n",
    "             'NorthernRockies': 'NR',\n",
    "             'Northwest': 'NW',\n",
    "             'OhioValley': 'OV',\n",
    "             'South': 'S',\n",
    "             'Southeast': 'SE',\n",
    "             'Southwest':'SW',\n",
    "             'UpperMidwest': 'UM',\n",
    "             'West': 'W',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b469d-b581-4ad7-bd2a-4cd3b0f9aaa5",
   "metadata": {},
   "source": [
    "## Utilities for data preparation 2021 & 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c044d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "aod_fv = -9999\n",
    "fill_value = np.nan\n",
    "is_subset = False\n",
    "koi = ['AODANA', 'BC', 'BLH', 'DU', 'ELEVATION', 'MAIAC', 'R', 'T2M', 'TCC', 'WIND_SPEED']\n",
    "\n",
    "region_name = 'Northeast'\n",
    "region_abbr = abbr_dict_region.get(region_name)\n",
    "\n",
    "base_dir = f'/mnt/s3/Thesis/Data/TrainingData/{region_name}/Imputation/'\n",
    "\n",
    "raster_base_dir = f'/mnt/ebs/{region_name}/Reprojected/'\n",
    "\n",
    "hdf5_2021 = f\"/{base_dir}/HDF5/{region_name}_20210101_20211231_20241126.hdf5\"\n",
    "hdf5_2022 = f\"/{base_dir}/HDF5/{region_name}_20220101_20221231_20241126.hdf5\"\n",
    "hdf5_2023 = f\"/{base_dir}/HDF5/{region_name}_20230101_20231231_20241126.hdf5\"\n",
    "hdf5_2024 = f\"/{base_dir}/HDF5/{region_name}_20240101_20241231_20250218.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c11d17-03a6-4601-b61d-c6a946f00b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/s3/Thesis/Data/TrainingData/Northeast/Imputation/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76230a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data_with_nans(data, scalers):\n",
    "    channels = data.shape[-1]\n",
    "    scaled_data = np.full(data.shape, np.nan, dtype=np.float32)\n",
    "    \n",
    "    for i in range(channels):\n",
    "        print(f'Scaling channel {i}')\n",
    "        channel_data = data[:, :, :, i]\n",
    "        # Flatten the array and filter out NaN values for scaling\n",
    "        valid_data = channel_data[~np.isnan(channel_data)].reshape(-1, 1)\n",
    "        \n",
    "        # Transform valid data using the scaler fitted on training data\n",
    "        if valid_data.size > 0:\n",
    "            scaled_valid_data = scalers[i].transform(valid_data).flatten()\n",
    "            \n",
    "            # Assign the scaled values back to the appropriate positions in the scaled array\n",
    "            valid_indices = ~np.isnan(channel_data)\n",
    "            scaled_data[:, :, :, i][valid_indices] = scaled_valid_data\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "def scale_y(y_train, y_val, y_test):\n",
    "     # Apply binary mask to y_data, setting masked areas to NaN\n",
    "    y_train_masked = np.where((y_train == -9999) | (y_train == -1), np.nan, y_train)\n",
    "    # Initialize a scaler for y_data\n",
    "    y_scaler = StandardScaler()\n",
    "    # Flatten y_data, excluding NaN values\n",
    "    valid_y_train = y_train_masked[~np.isnan(y_train_masked)].reshape(-1, 1)\n",
    "    # Fit and transform the valid y_data\n",
    "    scaled_valid_y_train = y_scaler.fit_transform(valid_y_train)\n",
    "    # Create a full-sized array filled with NaNs to hold the scaled y_data\n",
    "    scaled_y_train = np.full(y_train.shape, np.nan)\n",
    "    # Place the scaled data back into the scaled_y_data array at the valid positions\n",
    "    scaled_y_train[~np.isnan(y_train_masked)] = scaled_valid_y_train.flatten()\n",
    "    # y_data_scaled now contains the scaled target variable, with areas outside the ROI preserved as NaN\n",
    "    y_train_scaled = scaled_y_train\n",
    "    \n",
    "    y_val_masked = np.where((y_val == -9999) | (y_val == -1), np.nan, y_val)\n",
    "    y_test_masked = np.where((y_test == -9999) | (y_test == -1), np.nan, y_test)\n",
    "    \n",
    "    valid_val_indices = ~np.isnan(y_val_masked)\n",
    "    y_val_flat = y_val_masked[valid_val_indices].reshape(-1, 1)\n",
    "    y_val_scaled = np.full(y_val_masked.shape, np.nan)\n",
    "    y_val_scaled[valid_val_indices] = y_scaler.transform(y_val_flat).flatten()\n",
    "    \n",
    "    # Flatten, scale, and reshape back the test target data\n",
    "    valid_test_indices = ~np.isnan(y_test_masked)\n",
    "    y_test_flat = y_test_masked[valid_test_indices].reshape(-1, 1)\n",
    "    y_test_scaled = np.full(y_test_masked.shape, np.nan)\n",
    "    y_test_scaled[valid_test_indices] = y_scaler.transform(y_test_flat).flatten()\n",
    "    return y_train_scaled, y_val_scaled, y_test_scaled, y_scaler\n",
    "\n",
    "\n",
    "def custom_split(x_data, y_data, miss_matrix, date_arr, month_arr, season_arr):\n",
    "    # Step 1: Splitting into train_val (80%) and test (20%)\n",
    "    split_idx_test = int(len(x_data) * 0.8)\n",
    "    \n",
    "    x_train_val = x_data[:split_idx_test]\n",
    "    x_test = x_data[split_idx_test:]\n",
    "    \n",
    "    y_train_val = y_data[:split_idx_test]\n",
    "    y_test = y_data[split_idx_test:]\n",
    "    \n",
    "    miss_matrix_train_val = miss_matrix[:split_idx_test]\n",
    "    miss_matrix_test = miss_matrix[split_idx_test:]\n",
    "    \n",
    "    dates_train_val = date_arr[:split_idx_test]\n",
    "    dates_test = date_arr[split_idx_test:]\n",
    "    \n",
    "    month_train_val = month_arr[:split_idx_test]\n",
    "    month_test = month_arr[split_idx_test:]\n",
    "    \n",
    "    season_train_val = season_arr[:split_idx_test]\n",
    "    season_test = season_arr[split_idx_test:]\n",
    "    \n",
    "    # Step 2: Splitting train_val into train (85% of train_val) and val (15% of train_val)\n",
    "    split_idx_val = int(len(x_train_val) * 0.85)\n",
    "    \n",
    "    x_train = x_train_val[:split_idx_val]\n",
    "    x_val = x_train_val[split_idx_val:]\n",
    "    \n",
    "    y_train = y_train_val[:split_idx_val]\n",
    "    y_val = y_train_val[split_idx_val:]\n",
    "    \n",
    "    miss_matrix_train = miss_matrix_train_val[:split_idx_val]\n",
    "    miss_matrix_val = miss_matrix_train_val[split_idx_val:]\n",
    "    \n",
    "    dates_train = dates_train_val[:split_idx_val]\n",
    "    dates_val = dates_train_val[split_idx_val:]\n",
    "    \n",
    "    month_train = month_train_val[:split_idx_val]\n",
    "    month_val = month_train_val[split_idx_val:]\n",
    "    \n",
    "    season_train = season_train_val[:split_idx_val]\n",
    "    season_val = season_train_val[split_idx_val:]\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test, miss_matrix_train, miss_matrix_val, miss_matrix_test, dates_train, dates_val, dates_test, month_train, month_val, month_test, season_train, season_val, season_test\n",
    "    \n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'fall'\n",
    "\n",
    "def iso_to_gregorian(iso_year, iso_week, iso_day):\n",
    "    \"\"\"\n",
    "    Convert ISO year, week, and day into a Gregorian (standard) date.\n",
    "    \"\"\"\n",
    "    fourth_jan = datetime(iso_year, 1, 4)\n",
    "    delta_days = iso_day - fourth_jan.isoweekday()\n",
    "    year_start = fourth_jan + timedelta(days=delta_days)\n",
    "    return year_start + timedelta(weeks=iso_week-1)\n",
    "\n",
    "\n",
    "\n",
    "def process_hdf5_file(hdf_path, koi, is_subset, aod_fv, fill_value):\n",
    "    datasets = {}\n",
    "    with h5py.File(hdf_path, 'r') as file:\n",
    "        print(f'Read the lat and long points for file {hdf_path}')\n",
    "        latitude = file['latitude'][:]\n",
    "        longitude = file['longitude'][:]\n",
    "\n",
    "        print('squaring the coordinates')\n",
    "        lat_square = np.square(latitude)\n",
    "        lon_square = np.square(longitude)\n",
    "\n",
    "        print('finding the product of lat and long')\n",
    "        lat_lon_product = np.multiply(latitude, longitude)\n",
    "\n",
    "        datasets = {'latitude': latitude, 'longitude': longitude, 'lat_lon_product': lat_lon_product}\n",
    "\n",
    "        time_data = [t.decode(\"utf-8\") if isinstance(t, bytes) else t for t in file['time'][:]]\n",
    "        time_datetimes = [datetime.strptime(t, '%Y%m%d') for t in time_data]\n",
    "\n",
    "        datasets['time'] = time_data  # Already in string format\n",
    "\n",
    "        # Calculate week numbers; this works for both subset and full dataset cases\n",
    "        weeks = [t.isocalendar().week for t in time_datetimes]\n",
    "\n",
    "        for name in file.keys():\n",
    "            if name in koi:\n",
    "                print(f'Processing variable: {name}')\n",
    "\n",
    "                dataset = file[name][:]\n",
    "\n",
    "                if name == 'AODANA':\n",
    "                    data_bm = np.where(np.isnan(dataset), 0, 1)\n",
    "\n",
    "                if name == 'MAIAC':\n",
    "                    if dataset[0,0,0] == -9999:\n",
    "                        print('-9999 exists in the array as first pixel')\n",
    "                        dataset = np.where(dataset == -9999, np.nan, dataset)\n",
    "                    # dataset = np.where(dataset == aod_fv, -1, dataset)\n",
    "                    dataset = np.where(np.isnan(dataset), -1, dataset)\n",
    "                    dataset = np.where(data_bm == 0, aod_fv, dataset)\n",
    "                    data_mask = ma.masked_where(dataset == aod_fv, dataset)\n",
    "                else:\n",
    "                    dataset = np.where(data_bm == 0, fill_value, dataset)\n",
    "                datasets[name] = dataset\n",
    "\n",
    "    return datasets, data_bm\n",
    "\n",
    "\n",
    "def get_data(hdf5_2021, hdf5_2022, koi, is_subset, aod_fv, fill_value):\n",
    "    process_start_time = time.time()\n",
    "    # Process each file\n",
    "    print('1. Create dataset dictionary.')\n",
    "    datasets_2021, data_bm_2021 = process_hdf5_file(hdf5_2021, koi, is_subset, aod_fv, fill_value)\n",
    "    datasets_2022, data_bm_2022 = process_hdf5_file(hdf5_2022, koi, is_subset, aod_fv, fill_value)\n",
    "\n",
    "    # Merge the datasets\n",
    "    spatial_keys = ['latitude', 'longitude', 'lat_lon_product']\n",
    "    time_key = 'time'\n",
    "\n",
    "    # Initialize empty dictionaries for the merged datasets and spatial data\n",
    "    datasets = {}\n",
    "    spatial_data = {}\n",
    "    \n",
    "    # Loop through keys in datasets_2021\n",
    "    for key in datasets_2021:\n",
    "        if key in datasets_2022:\n",
    "            if key not in spatial_keys and key != time_key:\n",
    "                # Concatenate data for keys present in both datasets, excluding spatial and time keys\n",
    "                print(f'Key exists and concatenated: {key}')\n",
    "                datasets[key] = np.concatenate((datasets_2021[key], datasets_2022[key]), axis=0)\n",
    "            elif key in spatial_keys:\n",
    "                # Directly assign spatial data from datasets_2021 to the spatial_data dictionary\n",
    "                # Assuming you want to keep spatial data from 2021 or handle it differently\n",
    "                spatial_data[key] = datasets_2021[key]\n",
    "\n",
    "    data_bm = np.concatenate((data_bm_2021, data_bm_2022), axis=0)\n",
    "\n",
    "    # Free memory from individual year datasets\n",
    "    del data_bm_2021, data_bm_2022\n",
    "    gc.collect()  # Explicitly clear the garbage\n",
    "\n",
    "    latitude = spatial_data['latitude']\n",
    "    longitude = spatial_data['longitude']\n",
    "\n",
    "    print('2. Split the data into x and y.')\n",
    "    time_steps = datasets['AODANA'].shape[0]\n",
    "\n",
    "    expanded_spatial_data = []\n",
    "    for key in spatial_data.keys():\n",
    "        expanded_data = np.repeat(spatial_data[key][np.newaxis, :, :], time_steps, axis=0)\n",
    "        print(f'spatial key {key} has shape {expanded_data.shape}')\n",
    "        masked_expanded_data = np.where(data_bm == 0, np.nan, expanded_data)\n",
    "        expanded_spatial_data.append(masked_expanded_data)\n",
    "\n",
    "    # Free memory from spatial data dictionary\n",
    "    del spatial_data\n",
    "    gc.collect()\n",
    "    \n",
    "    # Prepare feature data identified by feature_keys for stacking\n",
    "    feature_keys = [key for key in datasets.keys() if key not in spatial_keys + ['MAIAC'] + ['time']]\n",
    "    feature_data = [datasets[key] for key in feature_keys]\n",
    "    \n",
    "    # Stacking both feature data and expanded spatial data\n",
    "    cv_data = np.stack(feature_data + expanded_spatial_data, axis=-1)\n",
    "    y_data = datasets['MAIAC']  # Target variable\n",
    "\n",
    "    # Free memory from merged datasets\n",
    "    del datasets\n",
    "    gc.collect()\n",
    "\n",
    "    print('3. Create the miss matrix')\n",
    "    # Initialize the missing matrix with zeros\n",
    "    miss_matrix = np.zeros_like(y_data, dtype=int)\n",
    "    # Mark observed values as 1 (Observed AOD values are greater than 0 and not equal to -9999)\n",
    "    observed_mask = (y_data != -1) & (y_data != -9999)\n",
    "    miss_matrix[observed_mask] = 1\n",
    "\n",
    "    # Free memory from intermediate mask variables\n",
    "    del observed_mask\n",
    "    gc.collect()\n",
    "\n",
    "    print('4. Get the season and month arrays')\n",
    "    # Define the start date (January 1, 1990)\n",
    "    time_data = np.concatenate((datasets_2021['time'], datasets_2022['time']), axis=0)\n",
    "    date_arr = np.array([datetime.strptime(date,'%Y%m%d') for date in time_data])\n",
    "\n",
    "    # Free memory from raw time data\n",
    "    del time_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Extract month and season from the date array\n",
    "    month_arr = np.array([date.month for date in date_arr])\n",
    "    seasons = np.array([get_season(month) for month in month_arr])\n",
    "\n",
    "    # For seasons\n",
    "    # Map each season to an integer\n",
    "    season_mapping = {'winter': 0, 'spring': 1, 'summer': 2, 'fall': 3}\n",
    "    season_arr = np.array([season_mapping[season] for season in seasons])\n",
    "\n",
    "    x_data = cv_data\n",
    "\n",
    "    # Free memory from raw arrays\n",
    "    del cv_data, seasons\n",
    "    gc.collect()\n",
    "\n",
    "    num_samples, height, width, channels = x_data.shape\n",
    "    print('7. Split - train test and validation')\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, miss_matrix_y_train, miss_matrix_y_val, miss_matrix_y_test, d_train, d_val, d_test, month_train, month_val, month_test, season_train, season_val, season_test = custom_split(x_data, y_data, miss_matrix, date_arr, month_arr, season_arr)\n",
    "\n",
    "    # Free memory from unused arrays\n",
    "    del x_data, y_data, miss_matrix, date_arr, month_arr, season_arr\n",
    "    gc.collect()\n",
    "    \n",
    "    print('8. Perform scaling of x - train test and validation')\n",
    "    scalers = [StandardScaler() for _ in range(x_train.shape[-1])]\n",
    "    \n",
    "    # Adjust binary_mask to match x_train shape for broadcasting\n",
    "    binary_mask_expanded = np.broadcast_to(data_bm[0][np.newaxis, :, :, np.newaxis], x_train.shape).astype(np.float32)\n",
    "    \n",
    "    # Initialize the scaled array with NaNs\n",
    "    x_train_scaled = np.full(x_train.shape, np.nan, dtype=np.float32)\n",
    "    \n",
    "    # Scaling logic for each channel\n",
    "    for i in range(x_train.shape[-1]):\n",
    "        print(f'Processing channel {i}')\n",
    "        channel_data = x_train[:, :, :, i]\n",
    "        \n",
    "        # Mask with NaN where binary mask is 0\n",
    "        channel_data_masked = np.where(binary_mask_expanded[:, :, :, i] == 0, np.nan, channel_data)\n",
    "        \n",
    "        # Flatten the array and filter out NaN values for scaling\n",
    "        valid_data = channel_data_masked[~np.isnan(channel_data_masked)].reshape(-1, 1)\n",
    "        \n",
    "        # Fit and transform valid data using the scaler\n",
    "        if valid_data.size > 0:\n",
    "            scaler = scalers[i]\n",
    "            scaled_valid_data = scaler.fit_transform(valid_data).flatten()\n",
    "            \n",
    "            # Assign the scaled values back to the appropriate positions in the scaled array\n",
    "            valid_indices = ~np.isnan(channel_data_masked)\n",
    "            x_train_scaled[:, :, :, i][valid_indices] = scaled_valid_data\n",
    "\n",
    "        # Store the scaler for inverse transformation later, if needed\n",
    "        scalers[i] = scaler\n",
    "    \n",
    "    # Free memory from unscaled train data\n",
    "    del x_train\n",
    "    gc.collect()\n",
    "    \n",
    "    print('9. Scale the x test and validation with y-train scaler')\n",
    "    # Usage example for validation and test data:\n",
    "    x_val_scaled = scale_data_with_nans(x_val, scalers)\n",
    "    x_test_scaled = scale_data_with_nans(x_test, scalers)\n",
    "\n",
    "    # Free memory from unscaled validation and test data\n",
    "    del x_val, x_test\n",
    "    gc.collect()\n",
    "    \n",
    "    print('10. Perform scaling of y - train test and validation')\n",
    "   \n",
    "    # y_train, y_val, y_test, y_scaler = scale_y(y_train, y_val, y_test)\n",
    "    y_train_model = np.expand_dims(y_train, axis=1)\n",
    "    y_test_model = np.expand_dims(y_test, axis=1)\n",
    "    y_val_model = np.expand_dims(y_val, axis=1)\n",
    "    \n",
    "    miss_y_train_model = np.expand_dims(miss_matrix_y_train, axis=1)\n",
    "    miss_y_test_model = np.expand_dims(miss_matrix_y_test, axis=1)\n",
    "    miss_y_val_model = np.expand_dims(miss_matrix_y_val, axis=1)\n",
    "\n",
    "    # Free memory from unscaled target data\n",
    "    del y_train, y_val, y_test\n",
    "    gc.collect()\n",
    "\n",
    "    x_train_model = x_train_scaled.transpose(0, 3,  1, 2)  \n",
    "    x_test_model = x_test_scaled.transpose(0, 3,  1, 2)  \n",
    "    x_val_model = x_val_scaled.transpose(0, 3,  1, 2)  \n",
    "\n",
    "    process_end_time = time.time()\n",
    "    # Calculate the execution time for the current iteration and convert to minutes\n",
    "    process_time_minutes = (process_end_time - process_start_time) / 60\n",
    "    print(f'Execution time for pre-processing: {process_time_minutes:.2f} minutes')\n",
    "    return y_train_model, y_test_model, y_val_model, miss_y_train_model, miss_y_test_model, miss_y_val_model, x_train_model, x_test_model, x_val_model, d_train, d_test, d_val, month_train, month_test, month_val, season_train, season_test, season_val, data_bm, scalers, latitude, longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47efbde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Create dataset dictionary.\n",
      "Read the lat and long points for file //mnt/s3/Thesis/Data/TrainingData/UpperMidwest/Imputation//HDF5/UpperMidwest_20210101_20211231_20241126.hdf5\n",
      "squaring the coordinates\n",
      "finding the product of lat and long\n",
      "Processing variable: AODANA\n",
      "Processing variable: BC\n",
      "Processing variable: BLH\n",
      "Processing variable: DU\n",
      "Processing variable: ELEVATION\n",
      "Processing variable: MAIAC\n",
      "Processing variable: R\n",
      "Processing variable: T2M\n",
      "Processing variable: TCC\n",
      "Processing variable: WIND_SPEED\n",
      "Read the lat and long points for file //mnt/s3/Thesis/Data/TrainingData/UpperMidwest/Imputation//HDF5/UpperMidwest_20220101_20221231_20241126.hdf5\n",
      "squaring the coordinates\n",
      "finding the product of lat and long\n",
      "Processing variable: AODANA\n",
      "Processing variable: BC\n",
      "Processing variable: BLH\n",
      "Processing variable: DU\n",
      "Processing variable: ELEVATION\n",
      "Processing variable: MAIAC\n",
      "Processing variable: R\n",
      "Processing variable: T2M\n",
      "Processing variable: TCC\n",
      "Processing variable: WIND_SPEED\n",
      "Key exists and concatenated: AODANA\n",
      "Key exists and concatenated: BC\n",
      "Key exists and concatenated: BLH\n",
      "Key exists and concatenated: DU\n",
      "Key exists and concatenated: ELEVATION\n",
      "Key exists and concatenated: MAIAC\n",
      "Key exists and concatenated: R\n",
      "Key exists and concatenated: T2M\n",
      "Key exists and concatenated: TCC\n",
      "Key exists and concatenated: WIND_SPEED\n",
      "2. Split the data into x and y.\n",
      "spatial key latitude has shape (730, 992, 1192)\n",
      "spatial key longitude has shape (730, 992, 1192)\n",
      "spatial key lat_lon_product has shape (730, 992, 1192)\n",
      "3. Create the miss matrix\n",
      "4. Get the season and month arrays\n",
      "7. Split - train test and validation\n",
      "8. Perform scaling of x - train test and validation\n",
      "Processing channel 0\n",
      "Processing channel 1\n",
      "Processing channel 2\n",
      "Processing channel 3\n",
      "Processing channel 4\n",
      "Processing channel 5\n",
      "Processing channel 6\n",
      "Processing channel 7\n",
      "Processing channel 8\n",
      "Processing channel 9\n",
      "Processing channel 10\n",
      "Processing channel 11\n",
      "9. Scale the x test and validation with y-train scaler\n",
      "Scaling channel 0\n",
      "Scaling channel 1\n",
      "Scaling channel 2\n",
      "Scaling channel 3\n",
      "Scaling channel 4\n",
      "Scaling channel 5\n",
      "Scaling channel 6\n",
      "Scaling channel 7\n",
      "Scaling channel 8\n",
      "Scaling channel 9\n",
      "Scaling channel 10\n",
      "Scaling channel 11\n",
      "Scaling channel 0\n",
      "Scaling channel 1\n",
      "Scaling channel 2\n",
      "Scaling channel 3\n",
      "Scaling channel 4\n",
      "Scaling channel 5\n",
      "Scaling channel 6\n",
      "Scaling channel 7\n",
      "Scaling channel 8\n",
      "Scaling channel 9\n",
      "Scaling channel 10\n",
      "Scaling channel 11\n",
      "10. Perform scaling of y - train test and validation\n",
      "Execution time for pre-processing: 62.63 minutes\n"
     ]
    }
   ],
   "source": [
    "y_train_model, y_test_model, y_val_model, miss_y_train_model, miss_y_test_model, miss_y_val_model, x_train_model, x_test_model, x_val_model, d_train, d_test, d_val, month_train, month_test, month_val, season_train, season_test, season_val, data_bm, scalers, latitude, longitude = get_data(hdf5_2021, hdf5_2022, koi, is_subset, aod_fv, fill_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6a42c-ed01-47b4-8d51-2b9fd7d72076",
   "metadata": {},
   "source": [
    "## Write the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7b5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X and Y datasets\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_X_Train.pkl', 'wb') as f:\n",
    "    pickle.dump([x_train_model], f)\n",
    " \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_X_Test.pkl', 'wb') as f:\n",
    "    pickle.dump([x_test_model], f)\n",
    " \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_X_Validation.pkl', 'wb') as f:\n",
    "    pickle.dump([x_val_model], f)\n",
    "\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Y_Train.pkl', 'wb') as f:\n",
    "    pickle.dump([y_train_model, miss_y_train_model], f)\n",
    " \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Y_Test.pkl', 'wb') as f:\n",
    "    pickle.dump([y_test_model, miss_y_test_model], f)\n",
    " \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Y_Validation.pkl', 'wb') as f:\n",
    "    pickle.dump([y_val_model, miss_y_val_model], f)\n",
    "\n",
    "# Saving date arrays\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_DATE_TTV.pkl', 'wb') as f:\n",
    "    pickle.dump([d_train, d_test, d_val], f)\n",
    "\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Season_TTV.pkl', 'wb') as f:\n",
    "    pickle.dump([season_train, season_test, season_val], f)\n",
    "\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Month_TTV.pkl', 'wb') as f:\n",
    "    pickle.dump([month_train, month_test, month_val], f)\n",
    "\n",
    "# Saving binary mask\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_BM.pkl', 'wb') as f:\n",
    "    pickle.dump(data_bm, f)\n",
    "     \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Grid.pkl', 'wb') as f:\n",
    "    pickle.dump([latitude, longitude], f)\n",
    "    \n",
    "# After fitting the scalers to the training data\n",
    "for i, scaler in enumerate(scalers):\n",
    "    with open(f'{base_dir}/Pickle/X_scaler_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e863070-abdc-44fa-b4e1-e23732b4078c",
   "metadata": {},
   "source": [
    "## Prepare 2023 for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d94b0f-a1f5-4ace-9e9e-156de9a6b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_inference(hdf5_infer, koi, is_subset, aod_fv, fill_value):\n",
    "    process_start_time = time.time()\n",
    "    # Process each file\n",
    "    print('1. Create dataset dictionary.')\n",
    "    datasets_infer, data_bm = process_hdf5_file(hdf5_infer, koi, is_subset, aod_fv, fill_value)\n",
    "\n",
    "    # Merge the datasets\n",
    "    spatial_keys = ['latitude', 'longitude', 'lon_square', 'lat_square', 'lat_lon_product']\n",
    "    time_key = 'time'\n",
    "\n",
    "    # Initialize empty dictionaries for the merged datasets and spatial data\n",
    "    datasets = {}\n",
    "    spatial_data = {}\n",
    "    \n",
    "    for key in datasets_infer:\n",
    "        if key not in spatial_keys and key != time_key:\n",
    "            datasets[key] = datasets_infer[key]\n",
    "        elif key in spatial_keys:\n",
    "            spatial_data[key] = datasets_infer[key]\n",
    "\n",
    "\n",
    "    latitude = spatial_data['latitude']\n",
    "    longitude = spatial_data['longitude']\n",
    "    print(f'latitude shape {latitude.shape}')\n",
    "\n",
    "    print('2. Split the data into x and y.')\n",
    "    time_steps = datasets['AODANA'].shape[0]\n",
    "\n",
    "    expanded_spatial_data = []\n",
    "    for key in spatial_data.keys():\n",
    "        expanded_data = np.repeat(spatial_data[key][np.newaxis, :, :], time_steps, axis=0)\n",
    "        print(f'spatial key {key} has shape {expanded_data.shape}')\n",
    "        masked_expanded_data = np.where(data_bm == 0, np.nan, expanded_data)\n",
    "        expanded_spatial_data.append(masked_expanded_data)\n",
    "\n",
    "    # Prepare feature data identified by feature_keys for stacking\n",
    "    feature_keys = [key for key in datasets.keys() if key not in spatial_keys + ['MAIAC'] + ['time']]\n",
    "    feature_data = [datasets[key] for key in feature_keys]\n",
    "    \n",
    "    # Stacking both feature data and expanded spatial data\n",
    "    cv_data = np.stack(feature_data + expanded_spatial_data, axis=-1)\n",
    "    y_data = datasets['MAIAC']  # Target variable\n",
    "    \n",
    "    print('3. Create the miss matrix')\n",
    "    # Initialize the missing matrix with zeros\n",
    "    miss_matrix = np.zeros_like(y_data, dtype=int)\n",
    "    # Mark observed values as 1 (Observed AOD values are greater than 0 and not equal to -9999)\n",
    "    observed_mask = (y_data != -1) & (y_data != -9999)\n",
    "    miss_matrix[observed_mask] = 1\n",
    "    \n",
    "    print('4. Get the time dimension - get the season and month data')\n",
    "    time_data = datasets_infer['time']\n",
    "    date_arr = np.array([datetime.strptime(date,'%Y%m%d') for date in time_data])  \n",
    "    # Extract month and season from the date array\n",
    "    month_arr = np.array([date.month for date in date_arr])\n",
    "    seasons = np.array([get_season(month) for month in month_arr])\n",
    "    # For seasons - Map each season to an integer\n",
    "    season_mapping = {'winter': 0, 'spring': 1, 'summer': 2, 'fall': 3}\n",
    "    season_arr = np.array([season_mapping[season] for season in seasons])\n",
    "\n",
    "    print('6. Concatenate extra covariates to the X-variable')\n",
    "    x_data = cv_data\n",
    "    num_samples, height, width, channels = x_data.shape\n",
    "    \n",
    "    print('7. Perform scaling of x - train test and validation')\n",
    "    x_data_scaled = np.empty_like(x_data, dtype=np.float32)\n",
    "\n",
    "    for i in range(x_data_scaled.shape[-1]):\n",
    "        channel_data = x_data[:, :, :, i]\n",
    "        # Mask with NaN where binary mask is 0\n",
    "        channel_data_masked = np.where(channel_data == -9999, np.nan, channel_data)\n",
    "        # Flatten channel data, exclude NaN values for scaling\n",
    "        valid_indices = ~np.isnan(channel_data_masked)\n",
    "        valid_data = channel_data_masked[valid_indices].reshape(-1, 1)\n",
    "        # Transform valid data using the loaded scaler\n",
    "        scaled_valid_data = scalers[i].transform(valid_data)\n",
    "        # Prepare an array filled with NaNs for the scaled channel\n",
    "        scaled_channel = np.full(channel_data.shape, np.nan)\n",
    "        # Place scaled data back, using valid indices\n",
    "        scaled_channel[valid_indices] = scaled_valid_data.flatten()\n",
    "        x_data_scaled[:, :, :, i] = scaled_channel\n",
    "\n",
    "    y_data_model = np.expand_dims(y_data, axis=1)\n",
    "    miss_matrix_model = np.expand_dims(miss_matrix, axis=1)\n",
    "    \n",
    "    # Transpose X data to match the model's expected input shape if necessary\n",
    "    x_data_model = x_data_scaled.transpose(0, 3, 1, 2)  \n",
    "    \n",
    "    process_end_time = time.time()\n",
    "    process_time_minutes = (process_end_time - process_start_time) / 60\n",
    "    print(f'Execution time for pre-processing: {process_time_minutes:.2f} minutes')\n",
    "    \n",
    "    # Return the processed data ready for model inference\n",
    "    return y_data_model, miss_matrix_model, x_data_model, date_arr, month_arr, season_arr, data_bm, latitude, longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19516bce-e28b-41fb-985d-ee4b5e3c660e",
   "metadata": {},
   "source": [
    "## Get the x and y scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068734e4-e802-47d1-a0f6-860a28a9561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels = x_train_model.shape[1]\n",
    "scalers = []\n",
    "# Load each scaler\n",
    "for i in range(10):\n",
    "    with open(f'{base_dir}Pickle/V1/X_scaler_{i}.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "        scalers.append(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd69f7c2-f780-46d7-8bf0-c21d5cf7e378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Create dataset dictionary.\n",
      "Read the lat and long points for file //mnt/s3/Thesis/Data/TrainingData/Northeast/Imputation//HDF5/Northeast_20240101_20241231_20250218.hdf5\n",
      "squaring the coordinates\n",
      "finding the product of lat and long\n",
      "Processing variable: AODANA\n",
      "Processing variable: BC\n",
      "Processing variable: BLH\n",
      "Processing variable: DU\n",
      "Processing variable: ELEVATION\n",
      "Processing variable: MAIAC\n",
      "Processing variable: R\n",
      "Processing variable: T2M\n",
      "Processing variable: TCC\n",
      "Processing variable: WIND_SPEED\n",
      "latitude shape (1171, 987)\n",
      "2. Split the data into x and y.\n",
      "spatial key latitude has shape (365, 1171, 987)\n",
      "spatial key longitude has shape (365, 1171, 987)\n",
      "spatial key lat_lon_product has shape (365, 1171, 987)\n",
      "3. Create the miss matrix\n",
      "4. Get the time dimension - get the season and month data\n",
      "6. Concatenate extra covariates to the X-variable\n",
      "7. Perform scaling of x - train test and validation\n",
      "Execution time for pre-processing: 29.21 minutes\n"
     ]
    }
   ],
   "source": [
    "infer_year = 2024\n",
    "y_data_model, miss_matrix_model, x_data_model, date_arr, month_arr, season_arr, data_bm, latitude, longitude = get_data_inference(hdf5_2024, koi, is_subset, aod_fv, fill_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237849ea-a19a-4455-b6a1-83583f2b5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{base_dir}/Pickle/{region_abbr}_X_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump([x_data_model], f)\n",
    "\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Y_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump([y_data_model, miss_matrix_model], f)\n",
    "\n",
    "# Saving date arrays\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_DATE_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump([date_arr], f)\n",
    " \n",
    "# Saving binary mask\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_BM_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump(data_bm, f)\n",
    "     \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Grid_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump([latitude, longitude], f)\n",
    "    \n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Season_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump(season_arr, f)\n",
    "\n",
    "with open(f'{base_dir}/Pickle/{region_abbr}_Month_Infer_{infer_year}.pkl', 'wb') as f:\n",
    "    pickle.dump(month_arr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362dadd1-b8e5-4c82-b833-f71a536528c1",
   "metadata": {},
   "source": [
    "## Create npy files for traditional models - read the train, test, and validation pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71fb0300-47e0-483d-9edc-45da73780e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading x train\n",
      "Reading x test\n",
      "Reading x val\n",
      "Reading y train\n",
      "Reading y test\n",
      "Reading y val\n",
      "Reading date\n",
      "Reading season\n",
      "Reading month\n",
      "reading binary mask\n",
      "Reading latitude and longitude\n"
     ]
    }
   ],
   "source": [
    "print('Reading x train')\n",
    "with open(f'{base_dir}/Pickle/NE_X_Train.pkl', 'rb') as f:\n",
    "    x_train_model = pickle.load(f)\n",
    "\n",
    "print('Reading x test')\n",
    "with open(f'{base_dir}/Pickle/NE_X_Test.pkl', 'rb') as f:\n",
    "    x_test_model = pickle.load(f)\n",
    "\n",
    "print('Reading x val')\n",
    "with open(f'{base_dir}/Pickle/NE_X_Validation.pkl', 'rb') as f:\n",
    "    x_val_model = pickle.load(f)\n",
    "\n",
    "print('Reading y train')\n",
    "with open(f'{base_dir}/Pickle/NE_Y_Train.pkl', 'rb') as f:\n",
    "    y_train_model, miss_y_train_model = pickle.load(f)\n",
    "\n",
    "print('Reading y test')\n",
    "with open(f'{base_dir}/Pickle/NE_Y_Test.pkl', 'rb') as f:\n",
    "    y_test_model, miss_y_test_model = pickle.load(f)\n",
    "\n",
    "\n",
    "print('Reading y val')\n",
    "with open(f'{base_dir}/Pickle/NE_Y_Validation.pkl', 'rb') as f:\n",
    "    y_val_model, miss_y_val_model = pickle.load(f)\n",
    "\n",
    "print('Reading date')\n",
    "with open(f'{base_dir}/Pickle/NE_DATE_TTV.pkl', 'rb') as f:\n",
    "    d_train, d_test, d_val = pickle.load(f)\n",
    "\n",
    "print('Reading season')\n",
    "with open(f'{base_dir}/Pickle/NE_Season_TTV.pkl', 'rb') as f:\n",
    "    season_train, season_test, season_val = pickle.load(f)\n",
    "\n",
    "print('Reading month')\n",
    "with open(f'{base_dir}/Pickle/NE_Month_TTV.pkl', 'rb') as f:\n",
    "    month_train, month_test, month_val = pickle.load(f)\n",
    "\n",
    "print('reading binary mask')\n",
    "with open(f'{base_dir}/Pickle/NE_BM.pkl', 'rb') as f:\n",
    "    data_bm = pickle.load(f)\n",
    "\n",
    "print('Reading latitude and longitude')\n",
    "with open(f'{base_dir}/Pickle/NE_Grid.pkl', 'rb') as f:\n",
    "    latitude, longitude = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1aa8cc-81cf-4b51-96b9-98594b1063fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((496, 12, 1171, 987), (146, 12, 1171, 987), (88, 12, 1171, 987))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_model[0].shape, x_test_model[0].shape, x_val_model[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77914642-c138-483c-8ace-4fdbd540cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_model.shape, y_test_model.shape, y_val_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158d96a-d1b9-465f-906f-b266c93fd9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_y_train_model.shape, miss_y_test_model.shape, miss_y_val_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d234cb6-0b87-4329-98ea-a8b2fd1c7761",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train.shape, d_test.shape, d_val.shape, season_train.shape, season_test.shape, season_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ef16e-a1ce-4255-9c6e-e2987eb3566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bm.shape, latitude.shape, longitude.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cfdd2-f6ab-4f2c-916e-dc21e8195908",
   "metadata": {},
   "source": [
    "## Generate covariate train test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff933223-78eb-4c49-82fa-fd8a3a01aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Initialize dictionaries to hold the feature values arrays for each channel and each dataset type\n",
    "features_values_per_channel = {\n",
    "    'val': [],\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "# Define the datasets and their corresponding base names\n",
    "datasets = {\n",
    "    'val': x_val_model[0],\n",
    "    'train': x_train_model[0],\n",
    "    'test': x_test_model[0]\n",
    "}\n",
    "\n",
    "# Loop over each channel\n",
    "for channel in range(11):\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        feature_values = []\n",
    "        time, channels, height, width = dataset.shape\n",
    "        # Show progress for each channel and dataset\n",
    "        for t in tqdm(range(time), desc=f'Processing channel {channel+1}, {dataset_name} dataset, time steps'):\n",
    "            for y in range(height):\n",
    "                for x in range(width):\n",
    "                    feature_value = dataset[t, channel, y, x]\n",
    "                    feature_values.append(feature_value)\n",
    "\n",
    "        # Convert the list to a numpy array for the current channel\n",
    "        feature_values_array = np.array(feature_values)\n",
    "        features_values_per_channel[dataset_name].append(feature_values_array)\n",
    "\n",
    "        # Save the numpy array to disk for the current channel and dataset\n",
    "        np.save(f'{base_dir}/NPY/{dataset_name}_ch{channel+1}.npy', feature_values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491b837-f114-4406-95e4-16213c162ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the channel number you want to process\n",
    "channel = 0\n",
    "\n",
    "# Define the arrays to process and their corresponding filenames\n",
    "arrays = [\n",
    "    ('y_val_model', y_val_model, 'y_val.npy'),\n",
    "    ('y_train_model', y_train_model, 'y_train.npy'),\n",
    "    ('y_test_model', y_test_model, 'y_test.npy'),\n",
    "    ('miss_y_train_model', miss_y_train_model, 'miss_y_train.npy'),\n",
    "    ('miss_y_test_model', miss_y_test_model, 'miss_y_test.npy'),\n",
    "    ('miss_y_val_model', miss_y_val_model, 'miss_y_val.npy')\n",
    "]\n",
    "\n",
    "# Process each array and save the feature values\n",
    "for name, array, filename in arrays:\n",
    "    # Initialize a list to hold the feature values for the specified channel\n",
    "    feature_values = []\n",
    "\n",
    "    # Get the height and width from the array\n",
    "    height, width = array.shape[2], array.shape[3]\n",
    "\n",
    "    # Show progress for the specified channel\n",
    "    for t in tqdm(range(array.shape[0]), desc=f'Processing channel {channel+1}, time steps'):\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                # Extract the feature value for the specified channel, time step, and pixel location\n",
    "                feature_value = array[t, channel, y, x]\n",
    "                feature_values.append(feature_value)\n",
    "\n",
    "    # Convert the list to a numpy array for the specified channel\n",
    "    feature_values_array = np.array(feature_values)\n",
    "\n",
    "    # Save the numpy array to the specified file\n",
    "    np.save(f'{base_dir}/NPY/{filename}', feature_values_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4f3d8-621e-4aec-b002-fc186c63aa01",
   "metadata": {},
   "source": [
    "## Create ROI binary mask for train test and validation and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3b042-436b-4ed6-9328-34a1b04678e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape = y_train_model.shape\n",
    "test_shape = y_test_model.shape\n",
    "val_shape = y_val_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb2d0e-b330-4c7c-aa8f-630b8ab174c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reshape_and_flatten_mask(binary_mask, data_shape):\n",
    "    # Step 1: Expand the binary mask to add necessary dimensions\n",
    "    expanded_mask = np.expand_dims(binary_mask, axis=(0, 1))  # Shape becomes (1, 1, height, width)\n",
    "\n",
    "    # Step 2: Use broadcasting to match the shape of the data\n",
    "    expanded_mask = np.tile(expanded_mask, (data_shape[0], data_shape[1], 1, 1))  # Shape becomes (time, channels, height, width)\n",
    "\n",
    "    # Step 3: Flatten the expanded mask\n",
    "    flattened_mask = expanded_mask.flatten()\n",
    "    \n",
    "    return flattened_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2b777-37f1-420e-970c-608544cdc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bm = reshape_and_flatten_mask(data_bm[0], train_shape)\n",
    "test_bm = reshape_and_flatten_mask(data_bm[0], test_shape)\n",
    "val_bm = reshape_and_flatten_mask(data_bm[0], val_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50847acc-93d8-452d-89a6-357fa3c59ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{base_dir}/NPY/train_bm.npy', train_bm)\n",
    "np.save(f'{base_dir}/NPY/test_bm.npy', test_bm)\n",
    "np.save(f'{base_dir}/NPY/val_bm.npy', val_bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c516b0-5960-4de6-9d82-e9de5d972767",
   "metadata": {},
   "source": [
    "## check the shape of reshaped 1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cc597-8915-46b5-af43-3f7d4b7b776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_y_train_arr = np.load(f'{base_dir}/NPY/miss_y_train.npy')\n",
    "miss_y_test_arr = np.load(f'{base_dir}/NPY/miss_y_test.npy')\n",
    "miss_y_val_arr = np.load(f'{base_dir}/NPY/miss_y_val.npy')\n",
    "miss_y_train_arr.shape, miss_y_test_arr.shape, miss_y_val_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5986b-9bc8-48ef-8196-ecca5b92f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_arr = np.load(f'{base_dir}/NPY/y_train.npy')\n",
    "y_test_arr = np.load(f'{base_dir}/NPY/y_test.npy')\n",
    "y_val_arr = np.load(f'{base_dir}/NPY/y_val.npy')\n",
    "y_train_arr.shape, y_test_arr.shape, y_val_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515bcd78-2a1e-49b2-849f-005a6c5175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_arr = np.load(f'{base_dir}/NPY/train_ch1.npy')\n",
    "x_test_arr = np.load(f'{base_dir}/NPY/test_ch1.npy')\n",
    "x_val_arr = np.load(f'{base_dir}/NPY/val_ch1.npy')\n",
    "x_train_arr.shape, x_test_arr.shape, x_val_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ec454-e498-4487-8fad-d7d25288948f",
   "metadata": {},
   "source": [
    "## reshape to the original for the target variable and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5efef4-09f3-45f2-b3ea-560a74d83def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the 1D array back to the original 4D shape\n",
    "original_shape = (496, 1, 1171, 987)\n",
    "reshaped_array = y_train_arr.reshape(original_shape)\n",
    "y_mask_arr = np.where(reshaped_array == -9999, np.nan, reshaped_array)\n",
    "\n",
    "time_step = 0\n",
    "channel = 0\n",
    "\n",
    "# Extract the 2D slice for visualization\n",
    "image_slice = y_mask_arr[time_step, channel, :, :]\n",
    "\n",
    "# Plot the 2D slice\n",
    "plt.imshow(image_slice, cmap='viridis')\n",
    "plt.title(f'Channel {channel+1}, Time Step {time_step+1}')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba530c-737a-42cf-9c5d-bce619036050",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_mask_arr, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fde2bd-fc70-4d90-95a1-413aebcd03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(miss_y_train_arr, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a1ed2-7cd9-41d1-9983-f21ee5dd62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(miss_y_train_model[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b7c2f-8b4a-4ada-bb94-9b1f548cd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train, d_test, d_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b35b9-c44a-4be2-a3ef-71f4d41a38e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
